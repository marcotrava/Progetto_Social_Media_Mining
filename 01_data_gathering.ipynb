{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76555f91",
   "metadata": {},
   "source": [
    "# PROGETTO SOCIAL MEDIA MINING A.A. 2024/25 UNIMI\n",
    "## Reddit Loves Rolex?\n",
    "\n",
    "**NB:** Ho rimosso tutti gli output dal notebook. Ora il file √® completamente pulito e leggero, pronto per essere visualizzato su GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26358433",
   "metadata": {},
   "source": [
    "# Connessione alle API di Reddit\n",
    "\n",
    "Nel blocco di seguito ho configuarato la connessione a Reddit utilizzando **\"PRAW\"** (Python Reddit API Wrapper), la libreria che mi permette di interagire  con i contenuti pubblici di Reddit come post, commenti, utenti, subreddit, ecc...\n",
    "\n",
    "Prima di tutto ho cercato di stabilire una connessione con l'API, testando che tutto funzioni correttamente, stampando un post dal subreddit **\"r/rolex\"**.\n",
    "\n",
    "**Come ho ottenuto le API Key di Reddit?**\n",
    "\n",
    "1. Ho creato un account su Reddit: https://www.reddit.com/\n",
    "2. Sono andato su: https://www.reddit.com/prefs/apps\n",
    "3. Ho cliccato su ‚Äúcreate app‚Äù e selezionato il tipo \"script\"\n",
    "4. Ho inserito:\n",
    "   - Nome: \"Inserisci NOME\"\n",
    "   - Redirect URI: \"http://localhost\"\n",
    "5. Dopo la creazione ho ottenuto:\n",
    "   - \"client_id\"\n",
    "   - \"client_secret\"\n",
    "   - \"user_agent\"\n",
    "\n",
    "Con queste credenziali ho l'accesso ai contenuti pubblici senza l'obbligo di effettuare  il login con username e password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c17eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "client_id = ''\n",
    "client_secret = ''\n",
    "user_agent = ' '\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "\n",
    "for post in reddit.subreddit('rolex').hot(limit=1):\n",
    "    print(f\"TITOLO: {post.title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e0c52e",
   "metadata": {},
   "source": [
    "Dalla stampa del codice si nota che ho configurato nel modo corretto le API di Reddit stabilendo una connessione via \"praw\".\n",
    "\n",
    "Ho recuperato e stampato un post reale dal subreddit \"r/rolex\" dal titolo:\n",
    "\"Watch Verification Thread ‚Äì If you're uncertain if a Rolex is good/bad/fake‚Ä¶\"\n",
    "\n",
    "Il mio prossimo obiettivo e cercare e raccogliere commenti che contengono **\"gmt\", \"submariner\", \"datejust\",** ovvero modelli inconici della casa orologeria di Rolex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8973f3e",
   "metadata": {},
   "source": [
    "# Estrazione dei commenti Reddit contenenti i modelli Rolex (GMT, Submariner, DateJust)\n",
    "\n",
    "Nel prossimo blocco di codice eseguo una raccolta mirata di commenti nei subreddit **\"r/rolex\" e \"r/watches\"**.  \n",
    "L'obiettivo √® identificare e raccogliere solo quei commenti che contengono riferimenti espliciti ai tre modelli di Rolex oggetto della mia analisi:\n",
    "\n",
    "1. Cerco post recenti contenenti la parola \"Rolex\" in \"r/rolex\" e \"r/watches\".\n",
    "2. Per ciascun post analizzo tutti i commenti pubblici e filtro quelli che contengono almeno una delle parole chiave dei modelli GMT, Submariner e DateJust.\n",
    "3. Per ogni commento valido salvo:\n",
    "   - Autore del commento\n",
    "   - Modello menzionato (es. \"submariner\")\n",
    "   - Testo completo del commento\n",
    "   - Timestamp della pubblicazione (convertito in formato leggibile)\n",
    "4. Salvo tutto in un file CSV (**\"reddit_comments_rolex.csv\"**) per l'uso successivo per la costruzione del grafo per Gephi e l'analisi della mia rete.\n",
    "5. Inoltre ho messo un ritardo tra le richieste per evitate limiti di rate da parte di Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "subreddits = ['rolex', 'watches']\n",
    "keywords = ['gmt', 'submariner', 'datejust']\n",
    "search_queries = ['Rolex GMT', 'Rolex Submariner', 'Rolex DateJust', 'Rolex GMT-Master II', 'Rolex Hulk', 'Rolex Batman', 'Rolex Pepsi']\n",
    "limit_per_query = 30000  \n",
    "\n",
    "data = []\n",
    "seen_ids = set()  # Per evitare duplicati\n",
    "\n",
    "for sub in subreddits:\n",
    "    print(f\"üîç Subreddit: r/{sub}\")\n",
    "    for query in search_queries:\n",
    "        print(f\"   ‚Üí Query: {query}\")\n",
    "        for submission in reddit.subreddit(sub).search(query, sort=\"new\", limit=limit_per_query):\n",
    "            if submission.id in seen_ids:\n",
    "                continue\n",
    "            seen_ids.add(submission.id)\n",
    "\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            for comment in submission.comments.list():\n",
    "                if comment.body and comment.author:\n",
    "                    text = comment.body.lower()\n",
    "                    for model in keywords:\n",
    "                        if model in text:\n",
    "                            data.append({\n",
    "                                'author': comment.author.name,\n",
    "                                'model': model,\n",
    "                                'text': text,\n",
    "                                'created_utc': pd.to_datetime(comment.created_utc, unit='s')\n",
    "                            })\n",
    "            time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('reddit_comments_rolex.csv', index=False)\n",
    "print(f\"Completato: Commenti salvati: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2efe2ed",
   "metadata": {},
   "source": [
    "In questo modo ho cercato su:\n",
    "- **subreddits**: in quale community cercare i post Reddit (ha uno \"scope\" ristretto su Rolex e Watches)\n",
    "- **keywords**: cosa cercare nei commenti (il testo nei commenti dopo che li ha scaricati)\n",
    "- **search_queries**: cosa cercare nei post per trovarli pi√π facilmente (sia il titolo che dentro ai post)\n",
    "Ed ho salvato su un file .csv tutti i commenti raccolti, ovvero 5265"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f46eeb",
   "metadata": {},
   "source": [
    "Adesso eseguo un **salvataggio dei dizionari in formato JSON,** cos√¨ evito di dover rieseguire lo scraping da Reddit ad ogni esecuzione del notebook.\n",
    "I dati estratti vengono salvati in locale sotto forma di dizionari JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919afe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for d in data:\n",
    "    d['created_utc'] = d['created_utc'].isoformat()\n",
    "\n",
    "with open('reddit_comments_data.json', 'w') as comments_file:\n",
    "    json.dump(data, comments_file, indent=4)\n",
    "\n",
    "with open('seen_ids.json', 'w') as seen_file:\n",
    "    json.dump(list(seen_ids), seen_file, indent=4)\n",
    "\n",
    "print(\"Dizionari salvati correttamente in file JSON.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5333ebc",
   "metadata": {},
   "source": [
    "\"**reddit_data.json**\" contiene tutti i commenti estratti, con autore, testo, modello menzionato e timestamp.\n",
    "\"**seen_ids.json**\" contiene l‚Äôelenco degli ID dei post gi√† processati, utile per evitare duplicati nelle future esecuzioni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604569d1",
   "metadata": {},
   "source": [
    "# Analisi dei commenti per modello e subreddit\n",
    "Grazie al successivo codice conto il numero di commenti raccolti per ciascun modello di Rolex (gmt, submariner e datejust) in base al testo.\n",
    "Distinguo i subreddit da cui provengono i commenti da r/rolex o r/watches.\n",
    "Visualizzo i risultati in:\n",
    "- Una tabella testuale riepilogativa\n",
    "- Un grafico a barre che mostra la distribuzione dei commenti per modello e subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf38246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Conto i commenti per modello e subreddit\n",
    "model_counts = defaultdict(lambda: defaultdict(int))\n",
    "for _, row in df.iterrows():\n",
    "    subreddit = 'rolex' if 'rolex' in row['text'] else 'watches'\n",
    "    model_counts[subreddit][row['model']] += 1\n",
    "\n",
    "print(\"\\nRiepilogo commenti per modello e subreddit:\")\n",
    "for sub, models in model_counts.items():\n",
    "    for model, count in models.items():\n",
    "        print(f\"r/{sub:<8} ‚Üí {model:<12}: {count} commenti\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca5228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preparo i dati per il grafico\n",
    "data_for_plot = []\n",
    "for subreddit, models in model_counts.items():\n",
    "    for model, count in models.items():\n",
    "        data_for_plot.append({'Subreddit': subreddit, 'Modello': model.capitalize(), 'Commenti': count})\n",
    "\n",
    "df_counts = pd.DataFrame(data_for_plot)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for subreddit in df_counts['Subreddit'].unique():\n",
    "    subset = df_counts[df_counts['Subreddit'] == subreddit]\n",
    "    plt.bar(subset['Modello'] + f' ({subreddit})', subset['Commenti'], label=f\"r/{subreddit}\")\n",
    "\n",
    "plt.title(\"Numero di commenti per modello e subreddit\")\n",
    "plt.xlabel(\"Modello Rolex (per subreddit)\")\n",
    "plt.ylabel(\"Numero di commenti\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi con Google Trends\n",
    "\n",
    "**Analisi temporale dell'interesse per i modelli Rolex tramite Google Trends (2013‚Äì2025)**\n",
    "In questa sezione del progetto, voglio analizzare l'evoluzione dell'interesse pubblico (sul prezzo) per i tre di Rolex con un'ottica temporale di lungo periodo dal 2013 al 2025.\n",
    "\n",
    "**Obiettivo:**\n",
    "Voglio identificare i periodi caldi, in hype, dove  l'interesse online per questi modelli ha avuto picchi significativi. \n",
    "Voglio analizzare:\n",
    "- Quando gli utenti iniziano a partecipare alle discussioni.\n",
    "- Se c'√® una relazione tra aumento dell'interesse (es. legato a prezzi in salita) e un aumento del coinvolgimento, attivit√† o sentiment negativo nei commenti.\n",
    "\n",
    "**Metodologia:**\n",
    "Utilizzo la libreria pytrends per estrarre i dati da Google Trends, relativi alle query, cercando: **\"Rolex Submariner/GMT/Datejust price\"**\n",
    "\n",
    "**Analisi dei dati raccolti:**\n",
    "Andamento dell'interesse nel tempo su scala 0‚Äì100.\n",
    "Identificazione dei picchi di interesse come proxy di eventi significativi (es. aumenti di prezzo, lanci di nuovi modelli, hype mediatico).\n",
    "Esporto i dati in CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pytrends = TrendReq(hl='en-US', tz=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306677dc",
   "metadata": {},
   "source": [
    "# Rolex Submariner\n",
    "\n",
    "Inizio con con il raccogliete dati per quanto riguarda il modello Rolex Submariner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cabf55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Rolex Submariner price'\n",
    "pytrends.build_payload([model], cat=0, timeframe='2013-01-01 2025-04-15', geo='', gprop='')\n",
    "\n",
    "data_sub = pytrends.interest_over_time().reset_index()\n",
    "data_sub['model'] = 'Submariner'\n",
    "\n",
    "data_sub.to_csv(\"trend_rolex_submariner.csv\", index=False)\n",
    "print(\"File per Submariner salvato: trend_rolex_submariner.csv\")\n",
    "data_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74bb1cc",
   "metadata": {},
   "source": [
    "\n",
    "I dati ottenuti sono stati salvati nel file CSV **\"trend_rolex_submariner.csv\"**, che potr√† essere successivamente utilizzato per:\n",
    "- Visualizzazioni temporali\n",
    "- Analisi di correlazione con le menzioni su Reddit\n",
    "- Studio della popolarit√† nel tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216943ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\" Attendo 10 minuti prima della prossima richiesta per evitare il blocco delle troppe richieste da Google (errore 429)\")\n",
    "time.sleep(600) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55af4da",
   "metadata": {},
   "source": [
    "# Rolex GMT\n",
    "\n",
    "Inizio con con il raccogliete dati per quanto riguarda il modello Rolex GMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00184708",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Rolex GMT price'\n",
    "pytrends.build_payload([model], cat=0, timeframe='2013-01-01 2025-04-15', geo='', gprop='')\n",
    "\n",
    "data_gmt = pytrends.interest_over_time().reset_index()\n",
    "data_gmt['model'] = 'GMT'\n",
    "\n",
    "data_gmt.to_csv(\"trend_rolex_gmt.csv\", index=False)\n",
    "print(\"File per GMT salvato: trend_rolex_gmt.csv\")\n",
    "data_gmt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Attendo 10 minuti prima della prossima richiesta per evitare il blocco delle troppe richieste da Google (errore 429)\")\n",
    "time.sleep(600) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9715791",
   "metadata": {},
   "source": [
    "# Rolex Datejust\n",
    "\n",
    "Inizio con con il raccogliete dati per quanto riguarda il modello Rolex Datejust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac290d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Rolex Datejust price'\n",
    "pytrends.build_payload([model], cat=0, timeframe='2013-01-01 2025-04-15', geo='', gprop='')\n",
    "\n",
    "data_dj = pytrends.interest_over_time().reset_index()\n",
    "data_dj['model'] = 'Datejust'\n",
    "\n",
    "data_dj.to_csv(\"trend_rolex_datejust.csv\", index=False)\n",
    "print(\"File per Datejust salvato: trend_rolex_datejust.csv\")\n",
    "data_dj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b6580",
   "metadata": {},
   "source": [
    "# Confronto tra interesse globale e attivit√† Reddit ‚Äì Rolex Submariner\n",
    "\n",
    "Adesso voglio confrontare l‚Äôevoluzione dell‚Äôinteresse globale per il **Rolex Submariner**, misurato tramite Google Trends, con l‚Äôattivit√† della community su Reddit.\n",
    "\n",
    "**Grafico**\n",
    "- **Linea blu** (asse sinistro): media annuale del trend Google con valori da 0 a 100.\n",
    "- **Linea arancione tratteggiata** (asse destro): numero di commenti Reddit sul Submariner per anno\n",
    "\n",
    "**Obiettivo:**\n",
    "Verificare se l‚Äôaumento dell‚Äôinteresse pubblico globale coincide con una maggiore partecipazione e discussione nella community del Social Media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7312eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico i commenti Reddit\n",
    "df_reddit = pd.read_csv(\"reddit_comments_rolex.csv\")\n",
    "df_reddit['created_utc'] = pd.to_datetime(df_reddit['created_utc'])\n",
    "df_reddit['year'] = df_reddit['created_utc'].dt.year\n",
    "\n",
    "# Filtr per modello \"Submariner\" e conto i commenti per anno\n",
    "df_sub = df_reddit[df_reddit['model'].str.lower() == 'submariner'].copy()\n",
    "reddit_yearly = df_sub.groupby('year').size().reset_index(name='num_comments')\n",
    "\n",
    "# Carico il trend Google e calcolo la media annuale\n",
    "df_trend = pd.read_csv(\"trend_rolex_submariner.csv\")\n",
    "df_trend['date'] = pd.to_datetime(df_trend['date'])\n",
    "df_trend['year'] = df_trend['date'].dt.year\n",
    "annual_df = df_trend.groupby('year')['Rolex Submariner price'].mean().reset_index(name='avg_trend')\n",
    "annual_df.set_index('year', inplace=True)\n",
    "\n",
    "merged = annual_df.merge(reddit_yearly, left_index=True, right_on='year', how='left')\n",
    "merged.set_index('year', inplace=True)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.set_title(\"Google Trends vs Commenti Reddit Rolex Submariner\")\n",
    "\n",
    "ax1.plot(merged.index, merged['avg_trend'], color='blue', marker='o', label='Trend medio (Google)')\n",
    "ax1.set_xlabel(\"Anno\")\n",
    "ax1.set_ylabel(\"Google Trends (0‚Äì100)\", color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(merged.index, merged['num_comments'], color='orange', linestyle='--', marker='s', label='Commenti Reddit')\n",
    "ax2.set_ylabel(\"N. commenti Reddit\", color='orange')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "plt.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47742b52",
   "metadata": {},
   "source": [
    "Il grafico mostra una chiara crescita parallela a partire dal 2021, con un‚Äôesplosione dell‚Äôattivit√† Reddit nel 2024‚Äì2025, coincidente con un forte incremento anche del trend su Google, quindi capisco che effettivamente l'interesse generale coincide con l'attivit√† nella community di Reddit.\n",
    "\n",
    "Faccio lo stesso confronto per gli altri due modelli di Rolex: **GMT** e **DateJust**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad94758",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gmt\"\n",
    "trend_file = \"trend_rolex_gmt.csv\"\n",
    "trend_column = \"Rolex GMT price\"\n",
    "title = \"Google Trends vs Commenti Reddit - Rolex GMT\"\n",
    "\n",
    "df_model = df_reddit[df_reddit['model'].str.lower() == model_name].copy()\n",
    "reddit_yearly = df_model.groupby('year').size().reset_index(name='num_comments')\n",
    "\n",
    "df_trend = pd.read_csv(trend_file)\n",
    "df_trend['date'] = pd.to_datetime(df_trend['date'])\n",
    "df_trend['year'] = df_trend['date'].dt.year\n",
    "annual_df = df_trend.groupby('year')[trend_column].mean().reset_index(name='avg_trend')\n",
    "annual_df.set_index('year', inplace=True)\n",
    "\n",
    "merged = annual_df.merge(reddit_yearly, left_index=True, right_on='year', how='left')\n",
    "merged.set_index('year', inplace=True)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.set_title(title)\n",
    "ax1.plot(merged.index, merged['avg_trend'], color='blue', marker='o', label='Trend medio (Google)')\n",
    "ax1.set_xlabel(\"Anno\")\n",
    "ax1.set_ylabel(\"Google Trends (0‚Äì100)\", color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(merged.index, merged['num_comments'], color='green', linestyle='--', marker='s', label='Commenti Reddit')\n",
    "ax2.set_ylabel(\"N. commenti Reddit\", color='green')\n",
    "ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "plt.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8551ed",
   "metadata": {},
   "source": [
    "Anche qui grazie a questo confronto noto una correlazione positiva tra l‚Äôinteresse generale e il coinvolgimento diretto della community Reddit, specialmente negli anni pi√π recenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"datejust\"\n",
    "trend_file = \"trend_rolex_datejust.csv\"\n",
    "trend_column = \"Rolex Datejust price\"\n",
    "title = \"Google Trends vs Commenti Reddit - Rolex Datejust\"\n",
    "\n",
    "df_model = df_reddit[df_reddit['model'].str.lower() == model_name].copy()\n",
    "reddit_yearly = df_model.groupby('year').size().reset_index(name='num_comments')\n",
    "\n",
    "df_trend = pd.read_csv(trend_file)\n",
    "df_trend['date'] = pd.to_datetime(df_trend['date'])\n",
    "df_trend['year'] = df_trend['date'].dt.year\n",
    "annual_df = df_trend.groupby('year')[trend_column].mean().reset_index(name='avg_trend')\n",
    "annual_df.set_index('year', inplace=True)\n",
    "\n",
    "\n",
    "merged = annual_df.merge(reddit_yearly, left_index=True, right_on='year', how='left')\n",
    "merged.set_index('year', inplace=True)\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.set_title(title)\n",
    "ax1.plot(merged.index, merged['avg_trend'], color='blue', marker='o', label='Trend medio (Google)')\n",
    "ax1.set_xlabel(\"Anno\")\n",
    "ax1.set_ylabel(\"Google Trends (0‚Äì100)\", color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(merged.index, merged['num_comments'], color='purple', linestyle='--', marker='s', label='Commenti Reddit')\n",
    "ax2.set_ylabel(\"N. commenti Reddit\", color='purple')\n",
    "ax2.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "plt.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e14f25",
   "metadata": {},
   "source": [
    "Ed anche per il Rolex Datejust il grafico mostra un aumento parallelo, soprattutto a partire dal 2022, notanto che l'interesse per questo modello √® cresciuto non solo a livello di ricerche, ma anche in termini di discussione attiva nella community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d040e9e",
   "metadata": {},
   "source": [
    "# Conclusione finale tra Google Trend e Commenti Reddit\n",
    "\n",
    "In tutti e tre i casi, si osserva una relazione crescente nel tempo tra interesse di ricerca e volume di commenti.\n",
    "L‚Äôimpennata nei commenti Reddit tra il 2022 e il 2025 sembra rispecchiare, o in alcuni casi anticipare, i picchi osservati su Google.\n",
    "Il modello GMT ha mostrato la crescita pi√π esplosiva su Reddit, mentre il Datejust ha registrato un picco recente molto significativo.\n",
    "\n",
    "L‚Äôobiettivo di questa analisi era verificare se esistesse una corrispondenza tra un potenziale aumento dei prezzi, dedotto dall‚Äôinteresse crescente su Google, e l‚Äôincremento dell‚Äôinterazione nei commenti su Reddit legato a tre modelli del brand di lusso dell'orologeria Rolex.\n",
    "\n",
    "Tuttavia, la mancanza di uno storico accurato sui prezzi rappresenta un limite importante. Questo rende effettivamente impossibile stabilire con certezza una correlazione diretta tra le due variabili. Sebbene i dati mostrino una crescita parallela nello stesso arco temporale, **non mi √® possibile concludere in modo definitivo che l‚Äôuna influenzi l‚Äôaltr**a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec7eee3",
   "metadata": {},
   "source": [
    "# Analisi rete\n",
    "\n",
    "Dopo aver esaminato l‚Äôandamento temporale dell‚Äôinteresse per i modelli Rolex su Google e Reddit, il mio progetto prosegue con una nuova prospettiva: la **struttura della community**.\n",
    "\n",
    "Voglio analizzare non solo quanto si parla dei modelli Rolex, ma anche come gli utenti si connettono tra loro in base a:\n",
    "\n",
    "- Argomenti condivisi (es. modelli comuni)\n",
    "- Discussioni sotto gli stessi thread\n",
    "- Frequenza di partecipazione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6026451",
   "metadata": {},
   "source": [
    "# Costruzione del grafo bipartito: Utenti Reddit - Modelli Rolex\n",
    "\n",
    "Adesso passo alla costruzione di un grafo bipartito per visualizzare le relazioni tra utenti Reddit e modelli Rolex.\n",
    "\n",
    "Un insieme rappresenter√† gli utenti Reddit, mentre l‚Äôaltro insieme rappresenter√† i modelli Rolex (Submariner, GMT, Datejust).\n",
    "Un arco collega un utente a un modello se l‚Äôha menzionato almeno una volta nei commenti\n",
    "\n",
    "Tramite questo grafo posso vedere:\n",
    "- Quali modelli sono pi√π discussi (nodi ad alto grado)\n",
    "- Quali utenti parlano di pi√π modelli\n",
    "- La presenza di community  attorno a specifici orologi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "with open(\"reddit_comments_data.json\", \"r\") as comments_file:\n",
    "    data = json.load(comments_file)\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Aggiungo nodi utente e modello al grafo\n",
    "for comment in data:\n",
    "    user = comment['author']\n",
    "    model = comment['model']\n",
    "    \n",
    "    G.add_node(user, bipartite=0)   \n",
    "    G.add_node(model, bipartite=1)  \n",
    "    G.add_edge(user, model)         \n",
    "\n",
    "user_nodes = {n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "model_nodes = set(G) - user_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G, \"grafo_bipartito_reddit_rolex.gexf\")\n",
    "print(\"Grafo esportato in: grafo_bipartito_reddit_rolex.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf5f50e",
   "metadata": {},
   "source": [
    "![](grafo_bipartito.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d06e2",
   "metadata": {},
   "source": [
    "Il mio grafo bipartito √® composto da:\n",
    "- **Utenti Reddit** come un primo insieme di nodi\n",
    "- **Modelli Rolex** (`Submariner`, `GMT`, `Datejust`) come secondo insieme di nodi\n",
    "\n",
    "Un **arco collega** un utente a un modello se l‚Äôutente ha commentato almeno un post contenente riferimenti a quel modello.\n",
    "\n",
    "- Il **modello GMT** (nodo rosso in basso a destra) √® quello con il **maggior numero di utenti collegati**, indicando un interesse molto alto da parte della community.\n",
    "- Il modello **Datejust** (nodo rosso a sinistra) ha un cluster compatto di utenti, ma meno esteso rispetto al GMT.\n",
    "- Il grafo evidenzia **comunit√† di utenti** che si concentrano su uno o pochi modelli.\n",
    "- Alcuni utenti sono connessi a pi√π modelli, mostrando un comportamento trasversale nella discussione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff8607",
   "metadata": {},
   "source": [
    "# Proiezione del grafo bipartito sugli utenti\n",
    "\n",
    "Per capire meglio i comportamenti della mia community, effettuo la **proiezione del grafo bipartito** sugli utenti Reddit.  \n",
    "\n",
    "In questa rete:\n",
    "- Ogni **nodo** √® un utente\n",
    "- Un **arco** collega due utenti se hanno **commentato lo stesso modello Rolex**\n",
    "\n",
    "Questo mi consente di:\n",
    "- Rilevare **gruppi di utenti con interessi simili**\n",
    "- Identificare utenti con **maggiore centralit√†** (pi√π collegati ad altri)\n",
    "- Visualizzare le **relazioni indirette** tra utenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfdc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gexf(\"grafo_bipartito_reddit_rolex.gexf\")\n",
    "\n",
    "# Estraggo i nodi utente (bipartite = 0)\n",
    "utenti = [n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 0]\n",
    "\n",
    "G_utenti = nx.bipartite.projected_graph(G, utenti)\n",
    "\n",
    "nx.write_gexf(G_utenti, \"proiezione_utenti_rolex.gexf\")\n",
    "print(\"Proiezione utenti salvata: proiezione_utenti_rolex.gexf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb56d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Proiezione utenti Reddit:\")\n",
    "print(f\"- Nodi (utenti): {G_utenti.number_of_nodes()}\")\n",
    "print(f\"- Archi (connessioni): {G_utenti.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03544f8f",
   "metadata": {},
   "source": [
    "![](grafo_proiezione_utenti.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8aab41",
   "metadata": {},
   "source": [
    "# Analisi dei nodi pi√π connessi nella rete \n",
    "\n",
    "Dopo aver costruito e proiettato il grafo bipartito sugli utenti Reddit, ho applicato un filtro basato sul **grado** dei nodi per identificare gli utenti pi√π **\"centrali\"** nella rete.\n",
    "Quindi ho voluto isolare e visualizzare gli **utenti con grado massimo**, ovvero quelli che hanno il maggior numero di connessioni e interazioni condivise con altri utenti attraverso i modelli Rolex.\n",
    "\n",
    "- In **Gephi**, ho utilizzato il filtro **Topology > Degree Range** per selezionare i nodi con grado pari al valore massimo (in questo caso: `3215`).\n",
    "- I nodi ottenuti rappresentano utenti **altamente connessi**, potenzialmente **hub** della conversazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb6a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_grado = 3215\n",
    "utenti_con_grado_massimo = [(n, d) for n, d in G_utenti.degree if d == top_grado]\n",
    "\n",
    "print(f\"Utenti con grado {top_grado}: {len(utenti_con_grado_massimo)}\")\n",
    "for utente, grado in utenti_con_grado_massimo:\n",
    "    print(f\"- {utente} --> {grado} connessioni\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbc0f8",
   "metadata": {},
   "source": [
    "![](top_utenti_connessioni_3215.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969da9c6",
   "metadata": {},
   "source": [
    "# Analisi della mia rete utenti Rolex su Reddit\n",
    "\n",
    "Dopo aver costruito la proiezione degli utenti a partire dal grafo bipartito, procedo con un'analisi quantitativa della rete ottenuta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a951524",
   "metadata": {},
   "source": [
    "# Grado Minimo, Massimo e Grado Medio\n",
    "\n",
    "Un grado massimo elevato pu√≤ indicare la presenza di utenti estremamente attivi, mentre un grado minimo pu√≤ segnalare utenti poco connessi o marginali.\n",
    "Mentre il grado medio rappresenta il livello medio di connessione degli utenti nella rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = list(G_utenti.degree())\n",
    "\n",
    "grado_minimo = min(degrees, key=lambda x: x[1])[1]\n",
    "print(\"Grado minimo:\", grado_minimo)\n",
    "\n",
    "grado_massimo = max(degrees, key=lambda x: x[1])[1]\n",
    "print(\"Grado massimo:\", grado_massimo)\n",
    "\n",
    "grado_medio = sum(d for n, d in degrees) / len(degrees)\n",
    "print(\"Grado medio:\", grado_medio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9259d",
   "metadata": {},
   "source": [
    "# Densit√†\n",
    "\n",
    "La densit√† misura quanto il grafo √® connesso rispetto al massimo possibile, ed √® un valore che va da 0 ad 1. \n",
    "- 0 significa nessuna connessione tra i nodi (grafico completamente scollegato, solo nodi isolati).\n",
    "- 1 significa che tutti i nodi sono connessi tra loro (grafo completamente pieno, ogni utente collegato con tutti gli altri).\n",
    "Quindi per valori alti, vicino all'1, il grafo sar√† molto denso e con forte interazione; mentre per valori bassi la rete sar√† pi√π sparsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "densita = nx.density(G_utenti)\n",
    "print(\"Densit√† del grafo:\", densita)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f068a475",
   "metadata": {},
   "source": [
    "La densit√† appena trovata, ovvero 0.5729461170044026, √® relativamente alta, considerando che il grafo ha un numero considerevole di nodi. Posso dire che √® **compatta e ben connessa**, in cui oltre la met√† delle possibili connessioni sono effettivamente presenti, quindi mi indica che molti utenti si relazionano tra loro.\n",
    "Quindi effettivamente c'√® un forte interesse condiviso tra gli utenti nei confronti dei modelli Rolex analizzati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edeb842",
   "metadata": {},
   "source": [
    "# Moda del grado\n",
    "La moda √® il grado che si ripete pi√π frequentemente: indica il livello di attivit√† \"tipico\" nella rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_counts = {}\n",
    "for _, degree in degrees:\n",
    "    if degree in degree_counts:\n",
    "        degree_counts[degree] += 1\n",
    "    else:\n",
    "        degree_counts[degree] = 1\n",
    "\n",
    "most_common_degree = max(degree_counts, key=degree_counts.get)\n",
    "print(f\"Grado pi√π comune (moda): {most_common_degree}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0e2ab",
   "metadata": {},
   "source": [
    "# Mediana del grado\n",
    "\n",
    "La mediana aiuta a capire la distribuzione dei gradi. Se √® molto pi√π bassa della media, indica pochi nodi hub molto potenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradi = [d for _, d in degrees] \n",
    "gradi_ordinati = sorted(gradi) \n",
    "lunghezza = len(gradi_ordinati) \n",
    "\n",
    "if lunghezza % 2 == 1:\n",
    "    mediana = gradi_ordinati[lunghezza // 2]\n",
    "else:\n",
    "    indice_medio = lunghezza // 2\n",
    "    mediana = (gradi_ordinati[indice_medio - 1] + gradi_ordinati[indice_medio]) / 2\n",
    "\n",
    "print(f\"Mediana dei gradi: {mediana}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd88e42",
   "metadata": {},
   "source": [
    "Questo data significa che il 50% degli utenti ha un grado (numero di connessioni) inferiore o uguale a 2191, mentre l‚Äôaltro 50% ha un grado superiore o uguale.\n",
    "\n",
    "Poich√© la **mediana √® abbastanza alta**, considerando il massimo di 3215 connessioni possibili, deduco che:\n",
    "- La rete √® ben connessa, quinndi la maggior parte degli utenti ha un elevato numero di connessioni verso altri utenti.\n",
    "- Non √® una rete dominata da pochissimi super-nodi (hub), al contrario, molti utenti partecipano attivamente e stabiliscono numerosi collegamenti.\n",
    "\n",
    "Se la mediana fosse stata molto pi√π bassa rispetto alla media o al grado massimo, avremmo avuto una rete pi√π disomogenea, con pochi utenti fortemente centrali e molti periferici."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697cc6fc",
   "metadata": {},
   "source": [
    "# ECDF (Empirical Cumulative Distribution Function) dei gradi\n",
    "\n",
    "La funzione di distribuzione cumulativa empirica (ECDF) permette di visualizzare come sono distribuiti i gradi dei nodi nella rete degli utenti che commentano modelli Rolex su Reddit.\n",
    "L'asse delle ordinate rappresenta la frazione cumulativa di utenti con grado minore o uguale a un certo valore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "frazione_cumulativa = np.arange(1, len(gradi_ordinati) + 1) / len(gradi_ordinati)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(gradi_ordinati, frazione_cumulativa, marker='o', markersize=3, linestyle='-', color='purple')\n",
    "plt.xlabel('Grado del nodo')\n",
    "plt.ylabel('Frazione cumulativa')\n",
    "plt.title('ECDF dei gradi degli utenti (Reddit Rolex)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad820bbf",
   "metadata": {},
   "source": [
    "Dal grafico ECDF osservo che il network degli utenti Reddit interessati a Rolex presenta una struttura piuttosto coesa, con la maggior parte degli utenti √® connessa ai tre modelli tramite numerose interazioni. Solo una piccola parte risulta poco connessa o isolata. Difatti, pi√π del 70% degli utenti ha un grado superiore a 1000, indicando una forte partecipazione e interazione tra gli utenti.\n",
    "Inoltre, la presenza di nodi con grado molto elevato suggerisce l‚Äôesistenza di hub centrali, ovvero utenti particolarmente attivi o influenti all‚Äôinterno della rete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb63909",
   "metadata": {},
   "source": [
    "# Componenti connesse\n",
    "\n",
    "Voglio sapere se tutti gli utenti sono **\"parte della stessa rete\"** o se ci sono \"isole separate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aefdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_components = list(nx.connected_components(G_utenti))\n",
    "\n",
    "for i, component in enumerate(connected_components, 1):\n",
    "    num_utenti = len(component)\n",
    "    print(f\"Componente: {i} --> {num_utenti} utenti:\")\n",
    "    \n",
    "    # Scrivo i primi 10 utenti\n",
    "    utenti_lista = list(component)\n",
    "    print(\", \".join(utenti_lista[:10]))\n",
    "    if num_utenti > 10:\n",
    "        print(f\"+ altri {num_utenti-10} utenti\")\n",
    "\n",
    "num_components = len(connected_components)\n",
    "print(f\"\\nNumero totale di componenti connesse: {num_components}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062cd029",
   "metadata": {},
   "source": [
    "**Avendo una sola componente connessa** significa che l'intera rete √® strutturalmente unita, quindi la dimensione della componente pi√π grande: 3216 nodi, ovvero pari al numero totale di utenti.\n",
    "Non ci sono nodi isolati o gruppetti separati, pertanto l'interazione pu√≤ propagarsi liberamente tra qualsiasi coppia di utenti nella rete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50ff7d",
   "metadata": {},
   "source": [
    "# MISURE DI CENTRALIT√Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca673112",
   "metadata": {},
   "source": [
    "# Degree centrality \n",
    "La degree centrality √® una misura fondamentale per capire quanto un nodo √® connesso nel network rispetto agli altri.\n",
    "Pi√π un nodo ha connessioni, maggiore sar√† la sua centralit√† e quindi la sua influenza nella rete.\n",
    "\n",
    "Nel mio progetto:\n",
    "- Gli utenti Reddit pi√π centrali sono quelli che hanno commentato su pi√π modelli Rolex o che hanno collegamenti con pi√π utenti simili.\n",
    "- Un valore di degree centrality pi√π alto indica utenti pi√π attivi o influencer all'interno della community Rolex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deff35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality = nx.degree_centrality(G_utenti)\n",
    "\n",
    "# Ordino i nodi per centralit√† decrescente\n",
    "sorted_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)\n",
    "\n",
    "print(\"Primi 100 utenti per Degree Centrality:\")\n",
    "for node in sorted_nodes[:100]:\n",
    "    print(f\"Nodo: {node}, Degree Centrality: {degree_centrality[node]:}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b78865",
   "metadata": {},
   "source": [
    "Grazie alla mia Degree Centrality, noto che tanti utenti (ad esempio saloulkm, sporturaws, Euphoric_Ad_6091, ecc.) hanno una degree centrality pari quasi ad 1 (arrotondado).\n",
    "\n",
    "**Degree Centrality = 1** significa che questi utenti sono collegati a tutti gli altri utenti della rete, o quasi. Sono super connessi, dei veri e propri hub centrali!\n",
    "Poi si nota una caduta nei valori successivi, ad esempio:\n",
    "- Utenti come Morake122, legendaryrhirrany, hoo_haaa, ecc.(comunque sempre nei **primi 100**) hanno una degree centrality di 0.8547,\n",
    "Quindi sono ancora molto centrali, ma non completamente collegati a tutti gli altri nodi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ultimi 50 utenti per Degree Centrality:\")\n",
    "for node in sorted_nodes[-50:]:\n",
    "    print(f\"Nodo: {node}, Degree Centrality: {degree_centrality[node]:.5f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f232e054",
   "metadata": {},
   "source": [
    "Analizzando invece i **50 utenti con Degree Centrality pi√π bassa**, osservo che il loro valore si aggira intorno a 0.24479, indicando una posizione marginale nella rete. \n",
    "\n",
    "Questi utenti risultano meno connessi rispetto agli hub principali, suggerendo un coinvolgimento limitato o focalizzato su pochi modelli Rolex. \n",
    "Visivamente, questi nodi si posizionano ai margini della rete, lontani dal nucleo pi√π denso di connessioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f2856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_nodes = sorted_nodes[:10]\n",
    "bottom_10_nodes = sorted_nodes[-10:]\n",
    "\n",
    "nodi_top = [node for node  in top_10_nodes]\n",
    "valori_top = [degree_centrality[node] for node in top_10_nodes]\n",
    "\n",
    "nodi_bottom = [node for node in bottom_10_nodes]\n",
    "valori_bottom = [degree_centrality[node] for node in bottom_10_nodes]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.barh(nodi_top, valori_top, color='royalblue', label=\"Top 10 utenti\")\n",
    "\n",
    "ax.barh(nodi_bottom, valori_bottom, color='lightcoral', label=\"Ultimi 10 utenti\")\n",
    "\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel('Degree Centrality')\n",
    "plt.title('Top 10 vs Ultimi 10 utenti - Degree Centrality Reddit Rolex')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb07184",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_copy = G_utenti.copy()\n",
    "\n",
    "for node in G_copy.nodes():\n",
    "    if node in top_10_nodes:\n",
    "        G_copy.nodes[node]['tipo'] = 'top'\n",
    "    elif node in bottom_10_nodes:\n",
    "        G_copy.nodes[node]['tipo'] = 'bottom'\n",
    "    else:\n",
    "        G_copy.nodes[node]['tipo'] = 'other'\n",
    "\n",
    "nx.write_gexf(G_copy, \"grafo_reddit_rolextop_bottom.gexf\")\n",
    "\n",
    "print(\"Grafo esportato come grafo_reddit_rolextop_bottom.gexf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796c6ce",
   "metadata": {},
   "source": [
    "![](top_bottom_utenti.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10c231",
   "metadata": {},
   "source": [
    "# Distribuzione dei nodi\n",
    "\n",
    "Dopo a aver isolati i nodi dal resto vedo che:\n",
    "- I nodi **verdi** rappresentano i **Top 10 utenti**, quelli pi√π centrali, ad alto degree centrality. Sono molto connessi tra di loro e anche con altri nodi. Alcuni sono veri e propri \"hub\" che tengono unita la rete.\n",
    "- I nodi **rossi** rappresentano i **Bottom 10 utenti**, quelli meno centrali, con pochi collegamenti. Sono meno connessi, spesso connessi solo a uno o pochi nodi Top o tra di loro debolmente.\n",
    "\n",
    "Per quanto riguarda le connessioni, si nota che molti archi partono dai nodi \"Top\" verso altri nodi, mentre quelli \"Bottom\" hanno meno legami forti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edebf08",
   "metadata": {},
   "source": [
    "# Analisi della Betweenness Centrality per il mio grafo Reddit Rolex\n",
    "\n",
    "La betweenness centrality misura quanto spesso un nodo si trova sul cammino pi√π breve tra altri due nodi nella rete.\n",
    "\n",
    "Se un nodo ha alta betweenness, significa che √® un ponte importante tra gruppi di utenti.\n",
    "Se ha bassa betweenness (vicino a 0), non √® essenziale per collegare altri nodi: √® \"laterale\", non passa attraverso di lui l'informazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_centrality = nx.betweenness_centrality(G_utenti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc4962",
   "metadata": {},
   "source": [
    "Rispetto alla degree centrality calcolare la betweenness centrality richiede molto pi√π tempo. \n",
    "\n",
    "**Degree centrality** va a contare solo quanti archi ha un nodo, quindi l'operazione √® abbastanza veloce, perch√® basta scorrere tutti i vicini.\n",
    "**Betweenness centrality**, invece va a calcolare tutti i cammini minimi tra ogni coppia di nodi. Per ogni nodo, vede quanto spesso passa attraverso i cammini minimi. \n",
    "\n",
    "Questo significa risolvere migliaia di shortest paths e quindi il costo computazionale sar√† molto pi√π alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bccb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordino i nodi per betweenness decrescente\n",
    "sorted_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Lista completa degli utenti per Betweenness Centrality:\")\n",
    "for node, betweenness in sorted_betweenness:\n",
    "    print(f\"Nodo: {node}, Betweenness Centrality: {betweenness:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eda17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 utenti per Betweenness Centrality:\")\n",
    "for node, betweenness in sorted_betweenness[:10]:\n",
    "    print(f\"Nodo: {node}, Betweenness Centrality: {betweenness:}\")\n",
    "\n",
    "print(\"\\nUltimi 10 utenti per Betweenness Centrality:\")\n",
    "for node, betweenness in sorted_betweenness[-10:]:\n",
    "    print(f\"Nodo: {node}, Betweenness Centrality: {betweenness:}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2302f5a",
   "metadata": {},
   "source": [
    "# Closeness Centrality\n",
    "\n",
    "La closeness centrality misura quanto un nodo √® \"vicino\" a tutti gli altri nodi della rete.\n",
    "Un valore alto indica che l'utente pu√≤ raggiungere rapidamente gli altri utenti attraverso un numero minimo di passaggi, quindi √® ben connesso \"globalmente\".\n",
    "\n",
    "Voglio calcolare la closeness centrality per tutti gli utenti della mia rete, ordinandoli dal pi√π \"vicino\" al pi√π \"lontano\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness_centrality = nx.closeness_centrality(G_utenti)\n",
    "\n",
    "# Ordino i nodi per closeness decrescente\n",
    "sorted_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Tutti gli utenti ordinati per Closeness Centrality:\")\n",
    "for node, closeness in sorted_closeness:\n",
    "    print(f\"Nodo: {node}, Closeness Centrality: {closeness:}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 utenti per Closeness Centrality:\")\n",
    "for node, closeness in sorted_closeness[:10]:\n",
    "    print(f\"Nodo: {node}, Closeness Centrality: {closeness:}\")\n",
    "\n",
    "print(\"\\nUltimi 10 utenti per Closeness Centrality:\")\n",
    "for node, closeness in sorted_closeness[-10:]:\n",
    "    print(f\"Nodo: {node}, Closeness Centrality: {closeness:}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f3ba32",
   "metadata": {},
   "source": [
    "L'analisi della closeness centrality mostra che nella mia rete esiste un piccolo gruppo di utenti che pu√≤ raggiungere rapidamente il resto della rete, facilitando cos√¨ la comunicazione e la propagazione delle informazioni. \n",
    "Per√≤, ho trovato anche la presenza di utenti con valori bassi suggerisce anche l'esistenza di aree marginali della rete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2bfb6",
   "metadata": {},
   "source": [
    "# Transitivit√†\n",
    "\n",
    "La transitivit√† √® una misura che indica quanto la rete sia coesa, ossia quanto √® probabile che due nodi connessi a un nodo comune siano anche connessi tra loro.\n",
    "√à calcolata tramite il coefficient di clustering globale, che rappresenta la proporzione di triangoli effettivamente presenti rispetto a quelli potenzialmente formabili nella rete.\n",
    "\n",
    "- Alta transitivit√†: la rete ha una struttura fortemente coesa, formata da gruppi o comunit√† ben definite.\n",
    "- Bassa transitivit√†: la rete √® pi√π frammentata, con meno connessioni triangolari e una struttura quindi meno compatta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a89d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "transitivity = nx.transitivity(G_utenti)\n",
    "print(f\"Transitivit√† del grafo utenti: {transitivity:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025d102",
   "metadata": {},
   "source": [
    "L'analisi della transitivit√† mi ha restituito un valore di 0.9119, indicando un **livello molto alto di coesione tra gli utenti.** \n",
    "Questo significa che se due utenti sono collegati a un terzo utente, √® estremamente probabile che siano anche collegati tra loro. \n",
    "\n",
    "La rete mostra una forte tendenza alla formazione di gruppi chiusi e comunit√† compatte, suggerendo un'interazione intensiva e specifica su temi comuni relativi al mondo Rolex su Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17cc56d",
   "metadata": {},
   "source": [
    "# Clustering medio\n",
    "\n",
    "Il clustering medio misura quanto, in media, i vicini di un nodo sono tra loro connessi.\n",
    "Serve a capire se nella rete gli utenti tendono a formare piccoli gruppi o community compatte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2451e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_clustering = nx.average_clustering(G_utenti)\n",
    "print(f\"Clustering medio della rete utenti: {average_clustering:}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebadefd9",
   "metadata": {},
   "source": [
    "Il **clustering medio** della rete √® risultato essere pari a 0.9518, un valore anch'esso molto elevato che evidenzia la **forte tendenza degli utenti a formare gruppi chiusi**. \n",
    "All'interno della mia rete, i vicini di un utente sono spesso connessi anche tra loro, dando origine a comunit√† molto compatte. Tale struttura favorisce una rapida circolazione delle informazioni all'interno dei gruppi e suggerisce un elevato livello di omogeneit√† tematica e di interazione sociale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199eac6",
   "metadata": {},
   "source": [
    "![](Clustering_Coefficient.png)\n",
    "![](Clustering_Coefficient_2.png)\n",
    "![](report_clustering_coefficient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7dff97",
   "metadata": {},
   "source": [
    "In **Gephi** ho colorato i nodi in base al valore del loro clustering coefficient: nodi con valori simili vengono raggruppati e colorati in modo simile.\n",
    "\n",
    "La maggioranza dei nodi ha un c**lustering coefficient molto alto**, 1.0 ed √® indicato dal colore fucsia, ed √® pari all' **84,61**% dei nodi.\n",
    "Questo significa che la maggior parte degli utenti forma \"gruppi chiusi\", dove ogni utente √® molto collegato agli altri utenti del proprio gruppo.\n",
    "Gli altri piccoli gruppi colorati (blu, verde, arancione) hanno valori leggermente pi√π bassi di clustering, distribuiti vicino a 0.5-0.8, sono minoranze quindi ed ogni gruppo rappresenta circa il **2%-6%** degli utenti.\n",
    "\n",
    "Dal grafico della distribuzione invece posso vedere che quasi tutto il conteggio si concentra vicino a 1, confermando che la rete √® altamente clusterizzata, quindi la stragrande maggioranza dei nodi forma triangoli chiusi, quindi gli utenti tendono a connettersi con amici di amici.\n",
    "\n",
    "L'analisi del clustering coefficient mostra che nella mia rete, ho una struttura fortemente comunitaria della rete, con pochi utenti che agiscono come ponti tra comunit√† diverse, che posso individuarli come le \"code\" esterne nel mio grafo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf6c89",
   "metadata": {},
   "source": [
    "# Cammino Minimo Medio\n",
    "\n",
    "Il cammino minimo medio rappresenta la distanza media pi√π breve tra tutte le coppie di nodi nel grafo.\n",
    "Un valore vicino a 1 indica che quasi tutti gli utenti sono direttamente connessi tra loro oppure che bastano pochissimi passaggi per raggiungere chiunque altro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac42b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_shortest_path_length = nx.average_shortest_path_length(G_utenti)\n",
    "\n",
    "print(\"Cammino minimo medio:\", average_shortest_path_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be06cfa",
   "metadata": {},
   "source": [
    "L'analisi del **cammino minimo medio** mostra che in media servono circa **1,43 passaggi** per collegare un qualsiasi utente a un altro nella mia rete. \n",
    "Questo valore molto basso evidenzia una struttura fortemente coesa, in cui l'informazione o le interazioni possono propagarsi rapidamente tra gli utenti.\n",
    "\n",
    "Questo dimostra ulteriormente con quello visto finora (clustering e transitivit√† alta, grafi compatti), quindi **la mia rete di utenti √® davvero connessa**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92564941",
   "metadata": {},
   "source": [
    "# Similiarit√†\n",
    "\n",
    "La similarit√† misura quanto due utenti sono \"vicini\" tra loro in base ai modelli di orologi di cui parlano nei commenti.\n",
    "In questo caso, ho utilizzato il coefficiente di similarit√† di Jaccard per confrontare i modelli associati agli utenti:\n",
    "\n",
    "- Valore 0: gli utenti non hanno mai commentato lo stesso modello di orologio.\n",
    "- Valore tra 0 e 1: maggiore √® il valore, pi√π alta √® la similarit√†, cio√® condividono modelli in comune.\n",
    "- Valore 1: gli utenti hanno commentato esattamente gli stessi modelli.\n",
    "\n",
    "Per cacolarla, carico il mio dizionario dal file json reddit_comments_data.json, dove sono stati estratti gli utenti e i modelli di orologi da loro commentati.\n",
    "Per ogni coppia di utenti, calcolo il coefficiente di Jaccard, che confronta l'intersezione e l'unione dei modelli commentati.\n",
    "Infine, i risultati li salvo in una matrice di similarit√†."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(set(list1).intersection(set(list2)))\n",
    "    union = len(set(list1).union(set(list2)))\n",
    "    return intersection / union if union != 0 else 0.0\n",
    "\n",
    "with open('reddit_comments_data.json', 'r') as f:\n",
    "    reddit_comments = json.load(f)\n",
    "\n",
    "# Costruisco dizionario utente -> lista di modelli commentati\n",
    "user_model_dict = {}\n",
    "for comment in reddit_comments:\n",
    "    user = comment['author']\n",
    "    model = comment['model']\n",
    "    if user in user_model_dict:\n",
    "        user_model_dict[user].append(model)\n",
    "    else:\n",
    "        user_model_dict[user] = [model]\n",
    "\n",
    "users = list(user_model_dict.keys())\n",
    "num_users = len(users)\n",
    "\n",
    "similarity_matrix = np.zeros((num_users, num_users))\n",
    "\n",
    "# Calcolo similarit√† per tutte le coppie\n",
    "for i, user1 in enumerate(users):\n",
    "    for j, user2 in enumerate(users):\n",
    "        if j > i:\n",
    "            models_user1 = set(user_model_dict[user1])\n",
    "            models_user2 = set(user_model_dict[user2])\n",
    "            similarity = jaccard_similarity(models_user1, models_user2)\n",
    "            similarity_matrix[i, j] = similarity\n",
    "            similarity_matrix[j, i] = similarity  \n",
    "\n",
    "np.savetxt('user_similarity_matrix.txt', similarity_matrix, delimiter='\\t')\n",
    "\n",
    "# Stampo alcune similarit√†\n",
    "for i, user1 in enumerate(users):\n",
    "    for j, user2 in enumerate(users):\n",
    "        if j > i and similarity_matrix[i, j] > 0:\n",
    "            print(f\"Similarit√† tra {user1} e {user2}: {similarity_matrix[i, j]:}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1703db",
   "metadata": {},
   "source": [
    "Visualizzo la **matrice di similarit√†** come una mappa a colori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "similarity_matrix = np.loadtxt('user_similarity_matrix.txt', delimiter='\\t')\n",
    "\n",
    "# Seleziono solo i primi 100 utenti per una visibilit√† migliore\n",
    "subset_size = 100\n",
    "small_matrix = similarity_matrix[:subset_size, :subset_size]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(small_matrix, cmap='viridis', cbar=True)\n",
    "plt.title('Mappa di Similarit√† tra i primi 100 utenti (Jaccard)', fontsize=16)\n",
    "plt.xlabel('Utenti')\n",
    "plt.ylabel('Utenti')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75081711",
   "metadata": {},
   "source": [
    "Ogni asse rappresenta un utente.\n",
    "\n",
    "Il colore di ogni cella, indica quanto sono simili tra loro l'utente \"i\" e l'utente \"j\", usando il coefficiente di similarit√† di Jaccard.\n",
    "\n",
    "- Colori pi√π chiari, come il giallo, indicano un' altissima similarit√† (vicina a 1.0), cio√® che gli utenti hanno commentato molti moelli in comune.\n",
    "- Colori scuri, blu o viola, invece indicano un bassa similarit√† (vicina a 0.0), quindi gli utenti hanno commentato modelli diversi tra loro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca7dde8",
   "metadata": {},
   "source": [
    "Adesso creo un nuovo **grafo degli utenti basato per√≤ sulla similarit√†**.\n",
    "\n",
    "Collego solo gli utenti che sono abbastanza simili, ad esempio se Jaccard > 0.5\n",
    "\n",
    "Prima per√≤ carico fil file json dove per ogni commento c'√® un \"author\", e questi author sono proprio gli utenti che vado ad estrarre e costruisco la mia lista users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_comments_data.json', 'r') as f:\n",
    "    reddit_comments = json.load(f)\n",
    "\n",
    "user_model_dict = {}\n",
    "for comment in reddit_comments:\n",
    "    user = comment['author']\n",
    "    model = comment['model']\n",
    "    if user in user_model_dict:\n",
    "        user_model_dict[user].add(model)\n",
    "    else:\n",
    "        user_model_dict[user] = {model}\n",
    "\n",
    "filtered_users = [user for user, models in user_model_dict.items() if len(models) >= 2]\n",
    "\n",
    "print(f\"Numero di utenti che hanno commentato almeno 2 modelli: {len(filtered_users)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.loadtxt('user_similarity_matrix.txt', delimiter='\\t')\n",
    "\n",
    "all_users = list(user_model_dict.keys())\n",
    "\n",
    "indices = [all_users.index(user) for user in filtered_users]\n",
    "filtered_similarity_matrix = similarity_matrix[np.ix_(indices, indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f74199",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_similarity_filtered = nx.Graph()\n",
    "\n",
    "for user in filtered_users:\n",
    "    G_similarity_filtered.add_node(user)\n",
    "\n",
    "# Aggiungo gli archi solo se la similarit√† supera una soglia alta pari a 0.7\n",
    "threshold = 0.7\n",
    "\n",
    "for i, user1 in enumerate(filtered_users):\n",
    "    for j, user2 in enumerate(filtered_users):\n",
    "        if j > i and filtered_similarity_matrix[i, j] > threshold:\n",
    "            G_similarity_filtered.add_edge(user1, user2, weight=filtered_similarity_matrix[i, j])\n",
    "\n",
    "nx.write_gexf(G_similarity_filtered, \"grafo_similarita_utenti_filtrato.gexf\")\n",
    "\n",
    "print(\"Grafo filtrato creato e salvato come 'grafo_similarita_utenti_filtrato.gexf'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb943b",
   "metadata": {},
   "source": [
    "Ho costruito una nuova rete dove i nodi rappresentano gli utenti Reddit, e gli archi esistono solo se la similarit√† tra gli utenti, calcolata tramite il coefficiente di Jaccard sui modelli Rolex commentati, supera una soglia di 0.7. \n",
    "Questa rete consente di visualizzare e analizzare cluster di utenti con gusti molto simili all'interno della community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29865a",
   "metadata": {},
   "source": [
    "![](similiaritaÃÄ_utenti.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bbe566",
   "metadata": {},
   "source": [
    "- **Nodi totali:** 495 utenti Reddit (in alto a destra).\n",
    "- **Archi totali:** 37.903 connessioni di similarit√†.\n",
    "- **4 componenti principali:**\n",
    "  - Blu: 40,4%\n",
    "  - Rosso: 32,7%\n",
    "  - Verde: 13,4%\n",
    "  - Rosa: 13,5%\n",
    "\n",
    "La mia rete evidenzia **quattro community principali** di utenti Reddit, ciascuna con **comportamenti simili** in termini di commenti sui modelli.\n",
    "- Gli utenti **dello stesso cluster** (stesso colore) tendono ad avere gusti simili: hanno commentato **gli stessi due modelli Rolex**.\n",
    "- La rete √® **densa**, segno che questi utenti sono altamente interconnessi tra loro.\n",
    "- La presenza di **pi√π componenti** mostra che ci sono sottogruppi abbastanza omogenei da essere distinti dagli altri, anche con soglie di similarit√† elevate.\n",
    "\n",
    "Questa visualizzazione conferma che gli utenti Reddit che commentano pi√π modelli Rolex tendono a formare **gruppi chiusi e coesi**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292efa5",
   "metadata": {},
   "source": [
    "# Community Detection\n",
    "\n",
    "Per identificare sottogruppi di utenti Reddit con comportamenti simili, adesso applico l'**algoritmo di rilevamento comunit√† basato sulla modularit√† Greedy**.  \n",
    "Con questo metodo cerco di massimizzare la **modularit√†**, un indice che misura quanto bene una rete pu√≤ essere suddivisa in comunit√† coese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64713209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "G = nx.read_gexf(\"grafo_similarita_utenti_filtrato.gexf\")\n",
    "\n",
    "# Rilevamento delle community con greedy modularity\n",
    "communities = community.greedy_modularity_communities(G)\n",
    "\n",
    "for i, group in enumerate(communities):\n",
    "    print(f\"Community {i+1} ({len(group)} utenti): {list(group)[:20]}...\")  # Mostra solo i primi 20 utenti per ogni gruppo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b80ceed",
   "metadata": {},
   "source": [
    "L‚Äôalgoritmo ha identificato **4 community**, ognuna composta da utenti che presentano **gusti simili**, ovvero hanno commentato **gli stessi modelli Rolex**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c77c78",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed677c8",
   "metadata": {},
   "source": [
    "# User Clustering by Comments\n",
    "\n",
    "## Creazione documento testuale per utuente basato sui commenti\n",
    "\n",
    "Creo il documento testuale per utente:\n",
    "- Andr√≤ a caricare i dati Reddit contenenti i commenti su modelli Rolex.\n",
    "- Creo un dizionario utente --> lista di commenti\n",
    "- Ogni utente sar√† rappresentato da un documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af21c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reddit_comments_data.json\", \"r\") as f:\n",
    "    rolex_data = json.load(f)\n",
    "\n",
    "# Dizionario utente -> lista di commenti\n",
    "user_comments = defaultdict(list)\n",
    "for entry in rolex_data:\n",
    "    user = entry[\"author\"].lower()\n",
    "    comment = entry[\"text\"]\n",
    "    user_comments[user].append(comment)\n",
    "\n",
    "# Genero documento testuale unico per ciascun utente\n",
    "user_ids = list(user_comments.keys())\n",
    "document4user = [\" \".join(comments) for comments in user_comments.values()]\n",
    "\n",
    "# Stampo un esempio di utente e il suo documento\n",
    "user_ids[7], document4user[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b6b6d",
   "metadata": {},
   "source": [
    "## Embedding dei commenti usando SpaCy\n",
    "\n",
    "Uso il modello en_core_web_md di spaCy per trasformare ogni documento in un vettore denso (embedding), dove ogni riga della matrice corrisponde a un utente e rappresenta il ‚Äúprofilo semantico‚Äù dei modelli menzionati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee144f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Carico modello SpaCy in inglese \n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Calcolo i vettori dei documenti\n",
    "X = np.array([nlp(doc).vector for doc in document4user])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90391fb4",
   "metadata": {},
   "source": [
    "## PCA (Riduzione della dimensionalit√†) \n",
    "\n",
    "Adesso applico la PCA per ridurre la dimensionalit√† dei vettori pur mantenendo almeno il 90% della varianza, per non perdere informazioni e cos√¨ rendo rendo dati pi√π maneggevoli per l‚Äôalgoritmo di clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "\n",
    "# Applico PCA mantenendo almeno il 90% della varianza\n",
    "pca = PCA(n_components=0.9, svd_solver='full')\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(StandardScaler(), pca, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance utilizzando PCA: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stampo la forma della matrice X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8643e",
   "metadata": {},
   "source": [
    "Ho quinndi **3216 utenti/documenti** (che corrispondno alle righe della matrice), dove ciascuno di essi √® rappresentato da un **vettore di 29 dimensioni** (le colonne), ovvero 29 componenti principali selezionate automaticamente dalla PCA per mantenere almeno il 90% della varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f264148",
   "metadata": {},
   "source": [
    "## Clustering con k-means\n",
    "\n",
    "Eseguo l‚Äôalgoritmo K-Means per raggruppare utenti simili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Eseguo clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42) # Ho scelto il numero di cluster pari a 4 poi controller√≤ con la Distorsione\n",
    "labels = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_vis = PCA(n_components=2)\n",
    "X_vis = pca_vis.fit_transform(X)\n",
    "                              \n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_vis[:, 0], X_vis[:, 1], c=labels, cmap=\"tab10\", alpha=0.7)\n",
    "plt.title(\"User Clustering by Comments - Rolex\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.colorbar(scatter, label=\"Cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "km = KMeans(n_clusters = 4, init='k-means++', max_iter=100, n_init=1)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f527c",
   "metadata": {},
   "source": [
    "Il grafico rappresenta una riduzione dimensionale tramite **PCA** dei vettori testuali dei commenti degli utenti Rolex.\n",
    "\n",
    "Ogni punto √® un utente, posizionato nello spazio in base alla somiglianza nei contenuti dei commenti.\n",
    "Il colore indica il cluster a cui l‚Äôutente √® stato assegnato dall‚Äô**algoritmo K-Means**.\n",
    "-\tUtenti nello stesso cluster hanno commenti simili.\n",
    "-\tCluster ben separati visivamente indicano gruppi omogenei, con comportamenti distinti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa72fdd1",
   "metadata": {},
   "source": [
    "## Distorsione \n",
    "\n",
    "Voglio analizzare l'andamento della distorsione al variare del numero di cluster da identificare.\n",
    "\n",
    "Calcolo la distorsione per diversi valori di k (ovvero numero di cluster), cos√¨ trovo il punto in cui l‚Äôattributo **inertia_** inizia a decrescere lentamente. \n",
    "Quel punto rappresenta un buon numero di cluster, bilanciando accuratezza e semplicit√†."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "K_range = range(2, 40, 2)  # Provo con un range ampio per vedere l'andamento\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)  \n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd84a5",
   "metadata": {},
   "source": [
    "Il grafico del metodo mostra che l‚Äôinertia decresce rapidamente fino a circa 4 cluster, per poi appiattirsi gradualmente. \n",
    "Quindi con **k = 4** √® un buon compromesso tra semplicit√† e qualit√† del clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c72a6a",
   "metadata": {},
   "source": [
    "## Silhoutte Coefficient \n",
    "\n",
    "Per verificare la **qualit√†** del clustering utilizzo la silhouette. La silhouette misura quanto un oggetto e' simile al cluster di appartenenza rispetto agli altri cluster.\n",
    "\n",
    "Il Silhouette varia tra -1 e 1.\n",
    "- Valori vicini a 1 indicano cluster ben separati.\n",
    "- Valori vicini a 0 indicano sovrapposizione.\n",
    "- Valori negativi indicano che molti punti sono assegnati al cluster sbagliato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f06f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette = silhouette_score(X, labels)\n",
    "print(f\"Silhouette Coefficient: {silhouette}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b957b25",
   "metadata": {},
   "source": [
    "## Visualizzazione dei commenti per cluster\n",
    "\n",
    "Tramite il mio dizionario user_comments, visualizzo i commenti raggruppati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e17352",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in sorted(set(labels)):\n",
    "    print(f\"\\n--- Cluster {c} ---\")\n",
    "    users_in_cluster = [u for u, lbl in zip(user_ids, labels) if lbl == c]\n",
    "    for u in users_in_cluster[:5]:  # Mostro solo i primi 5 utenti per cluster\n",
    "        print(f\"{u}: {user_comments.get(u, '')[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi dei Modelli pi√π Citati per Cluster\n",
    "\n",
    "Adesso voglio **associare ogni utente ad un cluster** e analizzre quali parole o hashtag sono pi√π frequenti all‚Äôinterno di ogni gruppo.\n",
    "\n",
    "- Considero ogni utente come un ‚Äúdocumento‚Äù composto dai modelli Rolex menzionati nei commenti.\n",
    "- Voglio anche vedere quali modelli vengono citati pi√π frequentemente in ogni cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d69e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = PCA(n_components=2)  \n",
    "pipeline = make_pipeline(StandardScaler(), pca)\n",
    "X = pipeline.fit_transform(X)  \n",
    "X = np.array([nlp(doc).vector for doc in document4user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "km = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "km.fit(X)\n",
    "\n",
    "# Creo IL dizionario user_id --> cluster\n",
    "user_to_cluster = dict(zip(user_ids, km.labels_))\n",
    "\n",
    "# Aggiungo anche i modelli associati a ciascun utente\n",
    "user_profile = {\n",
    "    k: {\n",
    "        'group': v,\n",
    "        'models': document4user[i].split()\n",
    "    }\n",
    "    for i, (k, v) in enumerate(user_to_cluster.items())\n",
    "}\n",
    "\n",
    "modelli_rolex ={ 'gmt', 'submariner', 'datejust', 'daytona', \n",
    "                'yachtmaster', 'explorer', 'sea-dweller', 'sky-dweller'\n",
    "                'day-date', 'milgauss', 'air-king', 'cellini',\n",
    "                'oyster-perpetual', 'pepsi', 'batman', 'hulk', 'rootbeer'\n",
    "                'airking', 'tudor', 'black-bay', 'pelagos', 'heritage'\n",
    "                }\n",
    "\n",
    "def hashtags_group(profiles, group, common=10):\n",
    "\n",
    "    all_models = defaultdict(set)\n",
    "    \n",
    "    for user, info in profiles.items():\n",
    "        if info['group'] == group:\n",
    "            for model in set(info['models']):  \n",
    "                model_clean = model.lower()\n",
    "                if model_clean in modelli_rolex: \n",
    "                    all_models[model_clean].add(user)\n",
    "    \n",
    "\n",
    "    model_counts = {model: len(users) for model, users in all_models.items()}\n",
    "    \n",
    "\n",
    "    return Counter(model_counts).most_common(common)\n",
    "\n",
    "for g in range(4):\n",
    "    print(f\"Cluster {g}:\")\n",
    "    print(hashtags_group(user_profile, g))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8fe6c",
   "metadata": {},
   "source": [
    "Ho quindi **analizzato quali modelli tra tutti i Rolex vengono menzionati pi√π frequentemente** da ciascun gruppo di utenti, cluster, secondo l'output dell'algoritmo di clustering.\n",
    "Questo mi aiuta a capire se ci sono gruppi di utenti con gusti e interessi simili verso determinati modelli Rolex.\n",
    "\n",
    "### Cluster 0\n",
    "- **Modelli pi√π menzionati:** gmt, datejust, explorer, pepsi, tudor, batman, daytona, hulk\n",
    "- **Interpretazione:** Gruppo variegato con interessi distribuiti su vari modelli popolari.\n",
    "- **Possibile profilo:** Appassionati generici di Rolex, con buona conoscenza del brand e interesse sia per modelli classici (es. datejust) sia sportivi (gmt, daytona, batman).\n",
    "\n",
    "### Cluster 1\n",
    "- **Modelli pi√π menzionati:** gmt, submariner, pepsi, explorer, daytona\n",
    "- **Interpretazione:** Cluster ristretto, focalizzato su pochi modelli iconici.\n",
    "- **Possibile profilo:** Utenti pi√π ‚Äúmainstream‚Äù o nuovi appassionati, interessati ai modelli Rolex pi√π noti.\n",
    "\n",
    "### Cluster 2\n",
    "- **Modelli pi√π menzionati:** gmt, submariner, tudor\n",
    "- **Interpretazione:** Molto compatto, con una forte concentrazione su due modelli principali.\n",
    "- **Possibile profilo:** Fan dei modelli professionali da immersione o sportivi, con gusti pi√π focalizzati.\n",
    "\n",
    "### Cluster 3\n",
    "- **Modelli pi√π menzionati:** gmt, submariner, tudor, explorer, daytona, batman, hulk, yachtmaster\n",
    "- **Interpretazione:** Il cluster pi√π ampio e diversificato.\n",
    "- **Possibile profilo:** Utenti con elevata conoscenza del brand, probabilmente esperti o collezionisti. Menzionano anche modelli pi√π ‚Äúdi nicchia‚Äù (yachtmaster).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c02ed1",
   "metadata": {},
   "source": [
    "# Sentiment Analysis dei commenti per modello di orologio\n",
    "\n",
    "La **sentiment analysis** √® una tecnica utile per valutare l‚Äôopinione espressa dagli utenti nei commenti. In questo caso, la applico ai commenti Reddit relativi a diversi modelli di orologi Rolex, per capire **quali modelli suscitano reazioni pi√π positive o negative** nella community.\n",
    "\n",
    "Utilizzer√≤ la libreria **TextBlob** per calcolare il sentiment, positivo o negativo, dei commenti associati a ciascun modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a53fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('reddit_comments_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity < 0:\n",
    "        return \"Negativo\"\n",
    "    else:\n",
    "        return \"Positivo\"\n",
    "\n",
    "# Dizionario modello -> lista di commenti\n",
    "model_comments = {}\n",
    "for entry in data:\n",
    "    model = entry.get('model')\n",
    "    text = entry.get('text', '')\n",
    "    if model and text:\n",
    "        model_comments.setdefault(model, []).append(text)\n",
    "\n",
    "# Analisi del sentiment per modello\n",
    "model_sentiments = {}\n",
    "for model, comments in model_comments.items():\n",
    "    sentiment_counts = {\"Positivo\": 0, \"Negativo\": 0}\n",
    "    for comment in comments:\n",
    "        sentiment = classify_sentiment(comment)\n",
    "        sentiment_counts[sentiment] += 1\n",
    "    model_sentiments[model] = sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, sentiments in model_sentiments.items():\n",
    "    print(f\"Modello: {model}\")\n",
    "    print(f\"  Commenti positivi: {sentiments['Positivo']}\")\n",
    "    print(f\"  Commenti negativi: {sentiments['Negativo']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "models = list(model_sentiments.keys())\n",
    "positive = [model_sentiments[m][\"Positivo\"] for m in models]\n",
    "negative = [model_sentiments[m][\"Negativo\"] for m in models]\n",
    "\n",
    "x = range(len(models))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x, positive, color='green', label='Positivi')\n",
    "plt.bar(x, negative, bottom=positive, color='red', label='Negativi')\n",
    "plt.xticks(x, models, rotation=45)\n",
    "plt.ylabel(\"Numero di commenti\")\n",
    "plt.title(\"Sentiment dei commenti per modello Rolex\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee259636",
   "metadata": {},
   "source": [
    "Noto che il modello **GMT** riceve di gran lunga il maggior numero di commenti, indicando un coinvolgimento molto alto nella mia community.\n",
    "Si vede che la **maggioranza dei commenti √® positiva per tutti e tre i modelli.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ce2ac",
   "metadata": {},
   "source": [
    "# Analisi del Sentiment per il modello \"Submariner\"\n",
    "\n",
    "Adesso andr√≤ a selezionare tutti i commenti associati al modello **Submariner** per fare un'esempio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dba78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_analyze = \"submariner\"\n",
    "model_comments = [entry for entry in data if entry[\"model\"].lower() == model_to_analyze.lower()]\n",
    "\n",
    "def classify_comment_sentiment(comment_text):\n",
    "    analysis = TextBlob(comment_text)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    return \"Positivo\" if polarity >= 0 else \"Negativo\"\n",
    "\n",
    "comment_sentiments = []\n",
    "for entry in model_comments:\n",
    "    sentiment = classify_comment_sentiment(entry[\"text\"])\n",
    "    comment_sentiments.append({\"author\": entry[\"author\"], \"comment\": entry[\"text\"], \"sentiment\": sentiment})\n",
    "\n",
    "for item in comment_sentiments:\n",
    "    print(f\"Utente: {item['author']}\")\n",
    "    print(f\"Commento: {item['comment']}\")\n",
    "    print(f\"Sentiment: {item['sentiment']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4898e18",
   "metadata": {},
   "source": [
    "## Risultato\n",
    "\n",
    "La maggior parte dei commenti per il Submariner risulta positiva come mi aspettavo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31e5e8",
   "metadata": {},
   "source": [
    "# ANALISI ALTRO MARCHIO PER PARAGONE\n",
    "\n",
    "Adesso voglio analizzare, per poi fare un paragone, un altro marchio di lusso dell'orologeria, ovvero **`Omega`**.\n",
    "Quindi il mio obiettivo √® estrarre i commenti, costruire un dataset per vedere la popolarit√† dei modelli, la community attorno al brand Omega e poi analizzare la similiarti√† nei comportamenti rispetto al brand **Rolex**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['watches','omegawatches']\n",
    "omega_keywords = ['speedmaster', 'seamaster', 'aqua terra']\n",
    "search_queries = [\n",
    "    'Omega Speedmaster', 'Omega Seamaster', 'Omega Aqua Terra',\n",
    "    'Omega Constellation', 'Speedy Tuesday', 'Moonwatch'\n",
    "]\n",
    "limit_per_query = 30000\n",
    "\n",
    "data = []\n",
    "seen_ids = set()\n",
    "\n",
    "for sub in subreddits:\n",
    "    print(f\"Subreddit: r/{sub}\")\n",
    "    for query in search_queries:\n",
    "        print(f\"   ‚Üí Query: {query}\")\n",
    "        for submission in reddit.subreddit(sub).search(query, sort=\"new\", limit=limit_per_query):\n",
    "            if submission.id in seen_ids:\n",
    "                continue\n",
    "            seen_ids.add(submission.id)\n",
    "\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            for comment in submission.comments.list():\n",
    "                if comment.body and comment.author:\n",
    "                    text = comment.body.lower()\n",
    "                    for model in omega_keywords:\n",
    "                        if model in text:\n",
    "                            data.append({\n",
    "                                'author': comment.author.name,\n",
    "                                'model': model,\n",
    "                                'text': text,\n",
    "                                'created_utc': pd.to_datetime(comment.created_utc, unit='s')\n",
    "                            })\n",
    "            time.sleep(1)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('reddit_comments_omega.csv', index=False)\n",
    "print(f\"Commenti salvati: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    d['created_utc'] = d['created_utc'].isoformat()\n",
    "\n",
    "with open('reddit_comments_omega.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "\n",
    "with open('seen_ids_omega.json', 'w') as seen_file:\n",
    "    json.dump(list(seen_ids), seen_file, indent=4)\n",
    "\n",
    "print(\"Dizionari JSON salvati correttamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f945e7",
   "metadata": {},
   "source": [
    " # Creo il grafo bipartito Omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e40bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_comments_omega.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "G_omega = nx.Graph()\n",
    "\n",
    "edges = {(entry['author'], entry['model']) for entry in data if entry['author'] and entry['model']}\n",
    "\n",
    "for author, model in edges:\n",
    "    G_omega.add_node(author, bipartite=0)\n",
    "    G_omega.add_node(model, bipartite=1)\n",
    "    G_omega.add_edge(author, model)\n",
    "\n",
    "nx.write_gexf(G_omega, \"grafo_bipartito_omega.gexf\")\n",
    "print(f\"Grafo bipartito Omega creato e salvato con {G_omega.number_of_nodes()} nodi e {G_omega.number_of_edges()} archi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e40758",
   "metadata": {},
   "source": [
    "![](bipartito_omega.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82049ecf",
   "metadata": {},
   "source": [
    "# Rete Bipartita: Utenti - Modelli Omega\n",
    "\n",
    "Ho costruito una rete bipartita in cui:\n",
    "- Un tipo di nodo rappresenta gli **utenti** Reddit che hanno commentato i modelli Omega.\n",
    "- L'altro tipo rappresenta i **modelli Omega** ovvero Speedmaster, Seamaster ed Aqua Terra.\n",
    "- Gli **archi** connettono ciascun utente ai modelli che ha commentato."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff358e73",
   "metadata": {},
   "source": [
    "# Proiezione del grafo bipartito Omega sugli utenti\n",
    "\n",
    "Per analizzare meglio la struttura della rete Omega, voglio proiettare il grafo bipartito sul set dei soli utenti. \n",
    "\n",
    "Cos√¨:\n",
    "- Ogni nodo rappresenta un utente.\n",
    "- Un arco esiste tra due utenti se hanno commentato almeno uno stesso modello Omega.\n",
    "- Il peso degli archi pu√≤ rappresentare la quantit√† di modelli commentati in comune.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed45c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nx.read_gexf(\"grafo_bipartito_omega.gexf\")\n",
    "\n",
    "utenti = [n for n, d in B.nodes(data=True) if d[\"bipartite\"] == 0]\n",
    "\n",
    "G_omega_user = nx.bipartite.projected_graph(B, utenti)\n",
    "\n",
    "nx.write_gexf(G_omega_user, \"proiezione_utenti_omega.gexf\")\n",
    "print(\"Proiezione utenti Omega salvata in 'proiezione_utenti_omega.gexf'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe67438",
   "metadata": {},
   "source": [
    "# Confronto tra la rete utenti Rolex e Omega\n",
    "\n",
    "Ho proiettato i grafi bipartiti di entrambi i brand sui soli utenti, ottenendo due reti, cos√¨ posso sia analizzare e confrontare la struttura delle community Reddit per ciascun marchio, che identificare quale rete risulta pi√π attiva e connessa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd988fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizza_rete(G, nome_rete=\"Rete\"):\n",
    "    return {\n",
    "        \"Rete\":nome_rete,\n",
    "        \"nodi\": G.number_of_nodes(),\n",
    "        \"archi\": G.number_of_edges(),\n",
    "        \"densit√†\": nx.density(G),\n",
    "        \"grado_medio\": sum(dict(G.degree()).values()) / G.number_of_nodes(),\n",
    "        \"componenti\": nx.number_connected_components(G),\n",
    "        \"diametro\": nx.diameter(G) if nx.is_connected(G) else \"non connessa\"\n",
    "    }\n",
    "\n",
    "analisi_rolex = analizza_rete(G_utenti, \"Rolex\")\n",
    "analisi_omega = analizza_rete(G_omega_user, \"Omega\")\n",
    "\n",
    "print(analisi_rolex)\n",
    "print(analisi_omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5175fc4",
   "metadata": {},
   "source": [
    "La rete **Rolex** √® pi√π attiva e interconnessa. Le interazioni sono pi√π frequenti e la distanza tra gli utenti √® minore, quindi tra le due  community, sembrerebbe quella pi√π coesa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee06c0",
   "metadata": {},
   "source": [
    "# Confronto utenti tra Rolex e Omega\n",
    "\n",
    "Adesso voglio confrontare le community Reddit dei marchi Rolex e Omega analizzando i commenti degli utenti.  \n",
    "Il mio obiettivo √® capire se esistono utenti che commentano entrambi i brand.\n",
    "\n",
    "Carico i due dizionari **utente --> modelli commentati** per ciascun marchio, poi faccio un'intersezione tra le chiavi dei due dizionari cos√¨ mi restituisce gli utenti in comune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aca067",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reddit_comments_data.json\", \"r\") as f:\n",
    "    rolex_data = json.load(f)\n",
    "\n",
    "with open(\"reddit_comments_omega.json\", \"r\") as f:\n",
    "    omega_data = json.load(f)\n",
    "\n",
    "# Creo i dizionari utente ‚Üí lista di modelli per entrambi i marchi di orologi\n",
    "rolex_user_model_dict = {}\n",
    "for entry in rolex_data:\n",
    "    user = entry[\"author\"]\n",
    "    model = entry[\"model\"]\n",
    "    rolex_user_model_dict.setdefault(user, []).append(model)\n",
    "\n",
    "omega_user_model_dict = {}\n",
    "for entry in omega_data:\n",
    "    user = entry[\"author\"]\n",
    "    model = entry[\"model\"]\n",
    "    omega_user_model_dict.setdefault(user, []).append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59745850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trovo gli utenti che hanno commentato entrambi i brand\n",
    "common_users = set(rolex_user_model_dict.keys()) & set(omega_user_model_dict.keys())\n",
    "\n",
    "print(f\"Utenti che hanno commentato sia Rolex che Omega: {len(common_users)}\")\n",
    "for user in list(common_users)[:20]:  \n",
    "    print(f\"\\nUtente: {user}\")\n",
    "    print(f\"  Modelli Rolex: {set(rolex_user_model_dict[user])}\")\n",
    "    print(f\"  Modelli Omega: {set(omega_user_model_dict[user])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d8551",
   "metadata": {},
   "source": [
    "# Grafo degli Utenti in Comune tra Rolex e Omega\n",
    "\n",
    "Voglio costruire un grafo per rappresentare gli utenti che hanno commentato sia modelli Rolex che Omega, dove ogni utente √® connesso ai modelli di entrambi i brand che ha commentato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e77e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_duale = nx.Graph()\n",
    "\n",
    "for user in common_users:\n",
    "    G_duale.add_node(user, bipartite=\"user\")\n",
    "\n",
    "for user in common_users:\n",
    "    for model in set(rolex_user_model_dict[user]):\n",
    "        node_name = f\"{model}\"\n",
    "        G_duale.add_node(node_name, bipartite=\"model\", brand=\"Rolex\")\n",
    "        G_duale.add_edge(user, node_name)\n",
    "        \n",
    "for user in common_users:\n",
    "    for model in set(omega_user_model_dict[user]):\n",
    "        node_name = f\"{model}\"\n",
    "        G_duale.add_node(node_name, bipartite=\"model\", brand=\"Omega\")\n",
    "        G_duale.add_edge(user, node_name)\n",
    "\n",
    "nx.write_gexf(G_duale, \"grafo_utenti_comuni_rolex_omega.gexf\")\n",
    "print(\"Grafo creato: grafo_utenti_comuni_rolex_omega.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9015c3",
   "metadata": {},
   "source": [
    "![](rolex_omega.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f6ecb0",
   "metadata": {},
   "source": [
    "# Confronto tra il Grafo Bipartito e la Proiezione sugli Utenti\n",
    "\n",
    "Nel mio progetto ho costruito due tipologie di grafo per analizzare le dinamiche tra utenti Reddit e i modelli di orologi dei brand Rolex e Omega:\n",
    "\n",
    "### 1. Grafo Bipartito (Utenti --> Modelli)\n",
    "\n",
    "- **Nodi**: utenti + modelli (es. *Submariner*, *Speedmaster*, ecc.)\n",
    "- **Archi**: collegano un utente a un modello che ha commentato.\n",
    "  - Capire quali modelli attirano pi√π attenzione.\n",
    "  - Analizzare la distribuzione degli utenti attorno ai diversi brand.\n",
    "  - Visualizzare le connessioni tra utenti e prodotti (es. utenti \"ibridi\").\n",
    "\n",
    "### 2. Proiezione sulla Rete degli Utenti\n",
    "\n",
    "- **Nodi**: solo utenti\n",
    "- **Archi**: due utenti sono collegati se hanno commentato almeno un modello in comune (anche tra brand diversi).\n",
    "  - Identificare comunit√† coese (community detection).\n",
    "  - Confrontare le strutture delle reti Rolex vs Omega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead1e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reddit_comments_data.json\", \"r\") as f:\n",
    "    rolex_data = json.load(f)\n",
    "with open(\"reddit_comments_omega.json\", \"r\") as f:\n",
    "    omega_data = json.load(f)\n",
    "\n",
    "rolex_dict = {}\n",
    "for entry in rolex_data:\n",
    "    rolex_dict.setdefault(entry[\"author\"], set()).add(entry[\"model\"])\n",
    "\n",
    "omega_dict = {}\n",
    "for entry in omega_data:\n",
    "    omega_dict.setdefault(entry[\"author\"], set()).add(entry[\"model\"])\n",
    "\n",
    "# Unisco gli  utenti\n",
    "all_users = set(rolex_dict) | set(omega_dict)\n",
    "\n",
    "brand_activity = {}\n",
    "for user in all_users:\n",
    "    rolex = user in rolex_dict\n",
    "    omega = user in omega_dict\n",
    "    if rolex and omega:\n",
    "        brand_activity[user] = \"Entrambi\"\n",
    "    elif rolex:\n",
    "        brand_activity[user] = \"Rolex\"\n",
    "    else:\n",
    "        brand_activity[user] = \"Omega\"\n",
    "\n",
    "G_pro_unito = nx.Graph()\n",
    "\n",
    "for user in all_users:\n",
    "    G_pro_unito.add_node(user, brand=brand_activity[user])\n",
    "\n",
    "user_list = list(all_users)\n",
    "for i in range(len(user_list)):\n",
    "    for j in range(i + 1, len(user_list)):\n",
    "        u1 = user_list[i]\n",
    "        u2 = user_list[j]\n",
    "\n",
    "        modelli_u1 = rolex_dict.get(u1, set()) | omega_dict.get(u1, set())\n",
    "        modelli_u2 = rolex_dict.get(u2, set()) | omega_dict.get(u2, set())\n",
    "\n",
    "        if modelli_u1 & modelli_u2:\n",
    "            G_pro_unito.add_edge(u1, u2)\n",
    "\n",
    "nx.write_gexf(G_pro_unito, \"proiezione_utenti_rolex_omega.gexf\")\n",
    "print(\"Proiezione utenti unificata salvata in 'proiezione_utenti_rolex_omega.gexf'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94215b",
   "metadata": {},
   "source": [
    "![](utenti_rolex_omega.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7265f0",
   "metadata": {},
   "source": [
    "- **Verde:** Utenti che commentano esclusivamente modelli Rolex.\n",
    "- **Rosso:** Utenti che commentano esclusivamente modelli Omega.\n",
    "- **Azzurro:** Utenti che commentano entrambi i brand.\n",
    "\n",
    "Ogni nodo rappresenta un utente e un link tra due nodi indica che hanno commentato almeno un modello in comune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b15c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G_pro_unito.degree())\n",
    "\n",
    "betweenness = nx.betweenness_centrality(G_pro_unito)\n",
    "\n",
    "data = []\n",
    "for node, attr in G_pro_unito.nodes(data=True):\n",
    "    brand = attr.get(\"brand\")\n",
    "    data.append({\n",
    "        \"user\": node,\n",
    "        \"brand\": brand,\n",
    "        \"degree\": degree_dict.get(node, 0),\n",
    "        \"betweenness\": betweenness.get(node, 0)\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(data)\n",
    "\n",
    "summary = df_metrics.groupby(\"brand\")[[\"degree\", \"betweenness\"]].mean().round(3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe3d5a",
   "metadata": {},
   "source": [
    "# PREDIRE QUALE BRAND TRA ROLEX E OMEGA LA GENTE PREFERISCE\n",
    "\n",
    "Il mio obiettivo √® **predire il brand preferito** di un utente. \n",
    "\n",
    "Quindi vorrei capire grazie ad un modello di **Machine Learning** se dato il comportamento testuale di un utente, riesco a predire se preferisce Rolex o Omega.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917aece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reddit_comments_data.json\", \"r\") as f:\n",
    "    rolex_data = json.load(f)\n",
    "\n",
    "with open(\"reddit_comments_omega.json\", \"r\") as f:\n",
    "    omega_data = json.load(f)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "rolex_users = {}\n",
    "for entry in rolex_data:\n",
    "    user = entry[\"author\"]\n",
    "    text = entry.get(\"text\", \"\")\n",
    "    if user not in rolex_users:\n",
    "        rolex_users[user] = {\"brand\": \"Rolex\", \"n_commenti\": 0, \"total_sentiment\": 0}\n",
    "    rolex_users[user][\"n_commenti\"] += 1\n",
    "    rolex_users[user][\"total_sentiment\"] += get_sentiment(text)\n",
    "\n",
    "omega_users = {}\n",
    "for entry in omega_data:\n",
    "    user = entry[\"author\"]\n",
    "    text = entry.get(\"text\", \"\")\n",
    "    if user not in omega_users:\n",
    "        omega_users[user] = {\"brand\": \"Omega\", \"n_commenti\": 0, \"total_sentiment\": 0}\n",
    "    omega_users[user][\"n_commenti\"] += 1\n",
    "    omega_users[user][\"total_sentiment\"] += get_sentiment(text)\n",
    "\n",
    "# Unisco i dizionari (ma solo utenti che hanno commentato un solo brand)\n",
    "all_users = {}\n",
    "for user, info in rolex_users.items():\n",
    "    if user not in omega_users:\n",
    "        all_users[user] = info\n",
    "for user, info in omega_users.items():\n",
    "    if user not in rolex_users:\n",
    "        all_users[user] = info\n",
    "\n",
    "df = pd.DataFrame.from_dict(all_users, orient=\"index\")\n",
    "df[\"avg_sentiment\"] = df[\"total_sentiment\"] / df[\"n_commenti\"]\n",
    "df = df[[\"n_commenti\", \"avg_sentiment\", \"brand\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8baf167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df[\"brand_label\"] = df[\"brand\"].map({\"Rolex\": 1, \"Omega\": 0})\n",
    "\n",
    "X = df[[\"n_commenti\", \"avg_sentiment\"]]\n",
    "y = df[\"brand_label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Modello\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Omega\", \"Rolex\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0210da0",
   "metadata": {},
   "source": [
    "# Valutazione del Modello di Classificazione\n",
    "\n",
    "Per valutare il modello di Machine Learning che predice se un utente preferisce Rolex o Omega, ho utilizzato:\n",
    "\n",
    "- **Accuracy**: percentuale di previsioni corrette, che corrisponde a 0.52, qui  il modello predice correttamente il brand in circa il 52% dei casi.\n",
    "- **Classification Report**: include **precision, recall e F1-score** per ciascuna classe.\n",
    "- **Confusion Matrix**: mostra il numero di predizioni corrette e sbagliate per ciascuna categoria, dove ho scelto **Rolex** come classe positiva.\n",
    "\n",
    "## Metriche:\n",
    "- **Precision:** mi indica quanti dei commenti classificati come appartenenti a un brand sono realmente corretti. Qui Rolex ha una precisione maggiore, 0,57, ma commette pi√π errori in recall.\n",
    "- **Recall:** mi misura la capacit√† del modello di identificare tutti i commenti realmente appartenenti a un brand. Qui Omega ottiene un valore pi√π alto ovvero 0.62.\n",
    "- **F1-score:** mi da la  media armonica tra precision e recall. Valore bilanciato leggermente migliore per Omega.\n",
    "- **Support:** √® il numero di esempi reali per ciascuna classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f64f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcolo la confusion matrix con ordine: [Omega, Rolex]\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1])  # 0 = Omega, 1 = Rolex\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Omega\", \"Rolex\"],\n",
    "            yticklabels=[\"Omega\", \"Rolex\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Rolex vs Omega\")\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23551246",
   "metadata": {},
   "source": [
    "In questa mia matrice di confusione:\n",
    "- **True Positives** (TP): 305 per Rolex.\n",
    "- **True Negatives** (TN): 390 per Omega.\n",
    "- **False Positives** (FP): 234 utenti che in realt√† preferivano Omega sono stati erroneamente classificati come Rolex, quindi il modello sovrastima Rolex. \n",
    "- **False Negatives** (FN): 402 utenti che preferivano Rolex sono stati erroneamente classificati come Omega.\n",
    "\n",
    "Questo modello tende quindi a sottostimare Rolex visto che commette pi√π errori nel riconoscere Rolex, con molti utenti che preferivano Rolex classificati come Omega, pertanto **Omega viene classificato meglio di Rolex**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffb5a1",
   "metadata": {},
   "source": [
    "# Curva ROC e AUC\n",
    "\n",
    "Per valutare la capacit√† del modello di distinguere tra i due brand voglio utilizzare la **ROC Curve (Receiver Operating Characteristic)** e il relativo **AUC (Area Under the Curve)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168475fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "y_prob = clf.predict_proba(X_test)[:, 1] \n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc = roc_auc_score(y_test, y_prob)  \n",
    "print(f\"AUC: {auc}\")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(fpr, tpr, linewidth=2, label=f\"ROC curve (AUC = {auc:.2f})\")\n",
    "ax.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
    "ax.set_xlabel('False Positive Rate - FPR')\n",
    "ax.set_ylabel('True Positive Rate (Recall)')\n",
    "ax.axis([0, 1, 0, 1])\n",
    "ax.legend(loc=\"lower right\", fontsize=13)\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36501cee",
   "metadata": {},
   "source": [
    "Il mio modello ha una capacit√† moderata di distinguere tra gli utenti che preferiscono Rolex e quelli che preferiscono Omega.\n",
    "- Un **AUC di 0.53** significa che, dato un esempio positivo, Rolex, e uno negativo, Omega, c‚Äô√® il 53% di probabilit√† che il modello assegni un punteggio pi√π alto al positivo. \n",
    "Quindi come gi√† visto nella matrice di confusione, il modello fatica a separare bene le due classi.\n",
    "- Non √® ottimo, perch√® √® leggermente migliore al caso casuale AUC = 0.50."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7217c7",
   "metadata": {},
   "source": [
    "# Predizione della Preferenza tra Rolex e Omega tramite Logistic Regression\n",
    "\n",
    "Il mio obiettivo adesso √® predire quale brand di orologi un utente preferisce tra Rolex e Omega, sfruttando per√≤ le caratteristiche strutturali della rete utente. Andr√≤ ad applicare un algoritmo di classificazione, quello di **Logistic Regression**.\n",
    "\n",
    "Utilizzo come variabili indipendenti sia caratteristiche testuali (numero di commenti e sentiment medio), sia caratteristiche strutturali della rete (degree e betweenness centrality), con l‚Äôobiettivo di migliorare la capacit√† predittiva del modello.\n",
    "\n",
    "Nel mio modello di regressione logistica,andr√≤ ad assegnare il valore 1 a Rolex e 0 a Omega. Questo significa che la regressione predice la probabilit√† che un utente preferisca Rolex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff62db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "\n",
    "df[\"brand_label\"] = df[\"brand\"].map({\"Omega\": 0, \"Rolex\": 1})\n",
    "\n",
    "X = df[[\"n_commenti\", \"avg_sentiment\", \"degree\", \"betweenness\"]]\n",
    "y = df[\"brand_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('minmax', MinMaxScaler(), [\"n_commenti\", \"avg_sentiment\", \"degree\", \"betweenness\"])\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('preprocessing', ct),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy:  {acc:}\")\n",
    "print(f\"Recall:    {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1:        {f1}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "coefs = model['classifier'].coef_\n",
    "print(f\"\\n{coefs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c801367",
   "metadata": {},
   "source": [
    "## Valutazione del Modello\n",
    "\n",
    "Il modello ha ottenuto una **accuracy del 63%**, con uno **F1 pari a circa 65%**. Questo indica che la regressione logistica √® in grado di discriminare, in modo moderato, tra utenti che preferiscono Rolex e quelli che preferiscono Omega.\n",
    "\n",
    "La **matrice di confusione** mostra il numero di predizioni corrette e incorrette per ciascuna classe e mostra che il modello riesce a identificare meglio gli utenti che preferiscono Rolex (469) rispetto a quelli che preferiscono Omega (367). Inoltre, i falsi positivi, Omega predetto come Rolex, sono pi√π alti dei falsi negativi.\n",
    "\n",
    "Dall'analisi dei **coefficienti del modello**:\n",
    "- Coefficienti positivi: spingono verso la classe positiva (Rolex)\n",
    "- Coefficienti negativi: spingono verso la classe negativa (Omega)\n",
    "\n",
    "Inoltre\n",
    "- **n_commenti** ha un impatto positivo, indicando che pi√π commenti possono indicare una preferenza per Rolex.\n",
    "- **betweenness** ha un impatto negativo marcato, suggerendo che utenti centrali nella rete preferiscono tendenzialmente Omega.\n",
    "\n",
    "Il modello **predice Rolex** quando:\n",
    "- l‚Äôutente ha molti commenti\n",
    "- ha un alto degree \n",
    "\n",
    "Il modello **predice Omega** quando:\n",
    "- l‚Äôutente ha sentiment molto positivo\n",
    "- ha alta betweenness "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c388629",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08638b24",
   "metadata": {},
   "source": [
    "### Analisi Confronto Omega vs Role: Preferenze e Coinvolgimento\n",
    "\n",
    "Identifico nuovamennte gli utenti che hanno commentato sia Rolex che Omega con degli esempi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577450cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolex_users = set([entry[\"author\"] for entry in rolex_data])\n",
    "omega_users = set([entry[\"author\"] for entry in omega_data])\n",
    "common_users = rolex_users.intersection(omega_users)\n",
    "\n",
    "print(f\"Utenti che hanno commentato entrambi i brand: {len(common_users)}\")\n",
    "print(\"Esempi:\", list(common_users)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b130a0",
   "metadata": {},
   "source": [
    "### Distribuzione utenti tra Rolex e Omega\n",
    "\n",
    "Vedo quale brand ha attratto pi√π utenti unici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Utenti unici Rolex: {len(rolex_users)}\")\n",
    "print(f\"Utenti unici Omega: {len(omega_users)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a533c",
   "metadata": {},
   "source": [
    "### Commenti per modello\n",
    "\n",
    "Analisi su quale modello √® pi√π discusso tra i brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "rolex_models = Counter([entry[\"model\"] for entry in rolex_data])\n",
    "omega_models = Counter([entry[\"model\"] for entry in omega_data])\n",
    "\n",
    "print(\"Modelli Rolex pi√π commentati:\", rolex_models.most_common(3))\n",
    "print(\"Modelli Omega pi√π commentati:\", omega_models.most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04cc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reddit_comments_data.json\", \"r\") as f:\n",
    "    rolex_data = json.load(f)\n",
    "with open(\"reddit_comments_omega.json\", \"r\") as f:\n",
    "    omega_data = json.load(f)\n",
    "\n",
    "user_model_map = defaultdict(set)\n",
    "\n",
    "# Inserisco i commenti Rolex\n",
    "for entry in rolex_data:\n",
    "    user_model_map[entry[\"author\"].lower()].add(entry[\"model\"].lower())\n",
    "\n",
    "# Inserisco i commenti Omega\n",
    "for entry in omega_data:\n",
    "    user_model_map[entry[\"author\"].lower()].add(entry[\"model\"].lower())\n",
    "\n",
    "modelli_target = [\"gmt\", \"submariner\", \"datejust\", \"speedmaster\", \"seamaster\", \"aqua terra\"]\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {model: int(model in models) for model in modelli_target}\n",
    "    for user, models in user_model_map.items()\n",
    "], index=list(user_model_map.keys()))\n",
    "\n",
    "def infer_brand_preference(row):\n",
    "    rolex = row[[\"gmt\", \"submariner\", \"datejust\"]].sum()\n",
    "    omega = row[[\"speedmaster\", \"seamaster\", \"aqua terra\"]].sum()\n",
    "    if rolex > omega:\n",
    "        return \"Rolex\"\n",
    "    elif omega > rolex:\n",
    "        return \"Omega\"\n",
    "    else:\n",
    "        return \"Equal\"\n",
    "\n",
    "df[\"brand_preference\"] = df.apply(infer_brand_preference, axis=1)\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentuali = df[\"brand_preference\"].value_counts(normalize=True) * 100\n",
    "print(percentuali.round(2).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca327753",
   "metadata": {},
   "source": [
    "# Sentiment \n",
    "\n",
    "Infine adesso brevemente analizzo il tono dei commenti, positivo, neutro oppure negativo, per ciascun brand cos√¨ da capire quale brand riceve sentiment pi√π favorevole da parte degli utenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61dab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unisco tutti i commenti in una lista unica con brand\n",
    "commenti = []\n",
    "\n",
    "for entry in rolex_data:\n",
    "    commenti.append({\"text\": entry[\"text\"], \"brand\": \"Rolex\"})\n",
    "\n",
    "for entry in omega_data:\n",
    "    commenti.append({\"text\": entry[\"text\"], \"brand\": \"Omega\"})\n",
    "\n",
    "def analizza_sentiment(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return \"Positivo\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negativo\"\n",
    "    else:\n",
    "        return \"Neutro\"\n",
    "\n",
    "for commento in commenti:\n",
    "    commento[\"sentiment\"] = analizza_sentiment(commento[\"text\"])\n",
    "\n",
    "df_sentiment = pd.DataFrame(commenti)\n",
    "\n",
    "report = df_sentiment.groupby(\"brand\")[\"sentiment\"].value_counts(normalize=True).unstack().round(2) * 100\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4475e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = {\n",
    "    'Rolex': {'Positivo': 68, 'Neutro': 17, 'Negativo': 15},\n",
    "    'Omega': {'Positivo': 68, 'Neutro': 21, 'Negativo': 11}\n",
    "}\n",
    "\n",
    "df_sentiment = pd.DataFrame(sentiment_counts).T\n",
    "\n",
    "df_percentuali = df_sentiment.div(df_sentiment.sum(axis=1), axis=0) * 100\n",
    "\n",
    "ax = df_percentuali.plot(kind='bar', stacked=True, figsize=(8, 5), color=[\"green\", \"gray\", \"red\"])\n",
    "\n",
    "plt.title(\"Distribuzione del Sentiment per Brand\")\n",
    "plt.ylabel(\"Percentuale (%)\")\n",
    "plt.xlabel(\"Brand\")\n",
    "plt.legend(title=\"Sentiment\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9922f3c6",
   "metadata": {},
   "source": [
    "Come mi aspettvo ho rilevato un sentiment molto simile verso Rolex e Omega, infatti i commenti sono prevalentemente positivi. C'√® una percentuale leggermente pi√π grande per i \"neutri\" per quanto riguarda Omega.\n",
    "\n",
    "Comunque, ho una percezione positiva e consolidata di entrambi i marchi nelle community analizzate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4181e0",
   "metadata": {},
   "source": [
    "# Metodi di Ensemble per la Predizione della Preferenza tra Rolex e Omega\n",
    "\n",
    "In questa sezione, ho voluto applicare **algoritmi di Ensemble Learning** per migliorare la capacit√† predittiva del mio modello, sfruttando combinazioni di pi√π classificatori deboli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b86697",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"n_commenti\", \"avg_sentiment\"]]\n",
    "y = df[\"brand_label\"]  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf9abc",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = RandomForestClassifier()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('lr', clf1), ('dt', clf2), ('rf', clf3)\n",
    "], voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "print(\"Accuracy Voting:\", accuracy_score(y_test, y_pred_voting))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62b3a1",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "print(\"Accuracy Bagging:\", accuracy_score(y_test, y_pred_bagging))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee7f1b",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "print(\"Accuracy Random Forest:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ff85a",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ed9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "y_pred_adaboost = adaboost_clf.predict(X_test)\n",
    "print(\"Accuracy AdaBoost:\", accuracy_score(y_test, y_pred_adaboost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47de04e",
   "metadata": {},
   "source": [
    "## Confronto tra modelli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Voting', 'Bagging', 'Random Forest', 'AdaBoost']\n",
    "accuracies = [\n",
    "    accuracy_score(y_test, y_pred_voting),\n",
    "    accuracy_score(y_test, y_pred_bagging),\n",
    "    accuracy_score(y_test, y_pred_rf),\n",
    "    accuracy_score(y_test, y_pred_adaboost)\n",
    "]\n",
    "\n",
    "colors = ['skyblue', 'lightgreen', 'salmon', 'plum']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models, accuracies, color=colors)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Confronto modelli Ensemble per predire preferenza tra Rolex e Omega\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b71048",
   "metadata": {},
   "source": [
    "# Metodi di Ensemble per la Predizione \n",
    "## Predire la Preferenza tra Rolex e Omega\n",
    "\n",
    "L‚Äôobiettivo √® predire, a partire da informazioni testuali e strutturali degli utenti su Reddit, se un utente preferisce **Rolex** o **Omega**. Utilizzeremo tre modelli:\n",
    "\n",
    "- **Decision Tree** (modello semplice, interpretabile)\n",
    "- **Random Forest** (ensemble di alberi, pi√π robusto)\n",
    "- **Bagging** (ensemble generico con Decision Tree)\n",
    "\n",
    "Useremo come feature:\n",
    "- Numero di commenti\n",
    "- Sentiment medio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f842d45",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = tree_clf.predict(X_train)\n",
    "y_test_pred = tree_clf.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Decision Tree train/test accuracy: {acc_train:.3f} / {acc_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2f6dc",
   "metadata": {},
   "source": [
    "L‚Äôaccuratezza bassa sia in train che in test indica che **l‚Äôalbero di decisione non riesce a catturare pattern significativi dai dati**.\n",
    "- il modello **non √® overfittato**, ma √® debole: probabilmente i dati non contengono sufficiente informazione predittiva oppure sono troppo rumorosi.\n",
    "- Performance vicina al random guessing, ovvero 0.5 quindi non √® utile per la predizione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfce7d6",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_rf = rf_clf.predict(X_train)\n",
    "y_test_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "acc_train_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "acc_test_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(f\"Random Forest train/test accuracy: {acc_train_rf:.3f} / {acc_test_rf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5303e1f6",
   "metadata": {},
   "source": [
    "Qui ho un‚Äô**accuratezza migliore**, segno che riesce a cogliere un po‚Äô di struttura nei dati.\n",
    "- Il test scende a 0.547, solo leggermente sopra il caso casuale.\n",
    "- L‚ÄôAUC visto prima era circa 0.53, coerente con questi numeri: il modello riesce a fare un po‚Äô meglio del caso, ma non abbastanza da poter dire che abbia potere predittivo reale.\n",
    "- La **prestazione migliora rispetto all‚Äôalbero** singolo ma ancora non affidabile per la classificazione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8db2c",
   "metadata": {},
   "source": [
    "## Bagging Classifier con Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9471bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=100,\n",
    "    max_samples=1.0,\n",
    "    max_features=1.0,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_bag = bagging_clf.predict(X_train)\n",
    "y_test_pred_bag = bagging_clf.predict(X_test)\n",
    "\n",
    "acc_train_bag = accuracy_score(y_train, y_train_pred_bag)\n",
    "acc_test_bag = accuracy_score(y_test, y_test_pred_bag)\n",
    "\n",
    "print(f\"Bagging train/test accuracy: {acc_train_bag:.3f} / {acc_test_bag:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958fcfc",
   "metadata": {},
   "source": [
    "**Accuracy quasi uguale a quella del decision tree singolo**.\n",
    "- Bagging aiuta a ridurre la varianza, ma in questo caso l‚Äôinstabilit√† del decision tree base non √® sufficiente per generare un guadagno evidente. \n",
    "- Risultato molto vicino al caso casuale, quindi anche qui poco utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b158d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Decision Tree', 'Random Forest', 'Bagging']\n",
    "\n",
    "# Accuracy dei test set,\n",
    "accuracies = [0.533, 0.547, 0.538]\n",
    "\n",
    "colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models, accuracies, color=colors)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Confronto modelli di classificazione\\nper predire preferenza tra Rolex e Omega\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617eaaa4",
   "metadata": {},
   "source": [
    "### **Nessuno dei modelli che ho testato ha purtroppo raggiunto un livello di accuratezza che dimostri una reale capacit√† predittiva**. \n",
    "Confrontando le accurac dei 3 modelli, anche se **Random Forest** ha una lievissima superiorit√† ,0.547, nessuno supera abbondantemente il caso casuale di 0.50\n",
    "- I dati disponibili non contengono abbastanza segnale predittivo per distinguere in modo affidabile chi preferisce Rolex o Omega.\n",
    "- Reddit ospita una community di appassionati di orologi, ma le preferenze espresse nei commenti non sono cos√¨ nette da poter essere codificate da un modello supervisionato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b83335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = df[[\"n_commenti\", \"avg_sentiment\"]]\n",
    "y = df[\"brand_label\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=42)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=5, random_state=1)\n",
    "bag = BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=5), n_estimators=100, random_state=1)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=1, max_features=1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "labels = ['Decision Tree', 'Bagging', 'Random Forest']\n",
    "\n",
    "for index, (clf, lab) in enumerate(zip([tree_clf, bag, rf_clf], labels)):\n",
    "    clf.fit(X_train, y_train)\n",
    "    f1_train = f1_score(y_train, clf.predict(X_train))\n",
    "    f1_test = f1_score(y_test, clf.predict(X_test))\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(1, 3, index + 1)\n",
    "    ax.set_title(f\"{lab}\\n train/test F1: {f1_train:.3f}/{f1_test:.3f}\")\n",
    "    plot_decision_regions(X=X_train, y=y_train.values, clf=clf, ax=ax, legend=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 4))\n",
    "labels = ['Decision Tree', 'Bagging', 'Random Forest']\n",
    "\n",
    "for index, (clf, lab) in enumerate(zip([tree_clf, bag, rf_clf], labels)):\n",
    "    clf.fit(X_train, y_train)\n",
    "    f1_train = f1_score(y_train, clf.predict(X_train))\n",
    "    f1_test = f1_score(y_test, clf.predict(X_test))\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, index + 1)\n",
    "    ax.set_title(f'{lab}\\n train/test F1: {f1_train:.3f}/{f1_test:.3f}')\n",
    "    \n",
    "    plot_decision_regions(X=X_train, y=y_train.values, clf=clf, ax=ax, legend=2)\n",
    "\n",
    " # Limitato con itervvallo sull'asse X\n",
    "    ax.set_xlim(0, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57212e56",
   "metadata": {},
   "source": [
    "Tre modelli di classificazione:\n",
    "- **Decision Tree**\n",
    "- **Bagging Classifier**\n",
    "- **Random Forest**\n",
    "\n",
    "Ho utilizzato come variabili esplicative:\n",
    "- **n_commenti**: numero di commenti lasciati dall‚Äôutente\n",
    "- **avg_sentiment**: sentimento medio associato ai commenti (standardizzato)\n",
    "- I dati li ho **normalizzati** con **StandardScaler()**\n",
    "- Ho **limitato l‚Äôintervallo degli assi X** a [0, 6] per evitare compressione visiva\n",
    "\n",
    "Risultati (F1 Score Train/Test):\n",
    "- **Decision Tree:** 0.636 / 0.608  \n",
    "- **Bagging:** 0.646 / 0.613  \n",
    "- **Random Forest:** 0.654 / 0.619  \n",
    "\n",
    "Il grafico mostra le *regioni di decisione* create da ciascun classificatore. Le zone blu rappresentano la classe 0 (Omega), quelle arancioni la classe 1 (Rolex).\n",
    "\n",
    "- **Decision Tree** tende ad overfittare leggermente, con confini pi√π rigidi e netti.\n",
    "- **Bagging** migliora la stabilit√†, rendendo le regioni pi√π coerenti.\n",
    "- **Random Forest** produce regioni frammentate, ma con una maggiore capacit√† predittiva sui dati test.\n",
    "\n",
    "L‚Äôuso di metodi ensemble come Bagging e Random Forest aumenta di poco l‚Äôaccuratezza predittiva sulla preferenza tra Rolex e Omega rispetto a un singolo albero decisionale, per√≤ non √® sufficiente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
